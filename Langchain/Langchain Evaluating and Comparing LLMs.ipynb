{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34764,
     "status": "ok",
     "timestamp": 1716062422639,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "RRYSu48huSUW",
    "outputId": "46102f01-7b11-486c-af0a-544bbefa11cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m938.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip -q install langchain huggingface_hub openai==0.27.2 google-search-results tiktoken cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ3fl8oyTLj4"
   },
   "source": [
    "\n",
    "## Cohere\n",
    "\n",
    "**Cohere is a cutting-edge AI platform that provides a suite of natural language processing (NLP) and natural language generation (NLG) tools and models. Cohere's mission is to enable developers to build more intelligent and human-like conversational interfaces, and to make it easier to generate high-quality, coherent, and engaging text.**\n",
    "\n",
    "Cohere's platform offers a range of features and capabilities, including:\n",
    "\n",
    "1. **Language Models**: Cohere provides a range of pre-trained language models that can be fine-tuned for specific tasks, such as text classification, sentiment analysis, and language translation.\n",
    "2. **Text Generation**: Cohere's platform allows developers to generate high-quality, coherent, and engaging text using a range of algorithms and models.\n",
    "3. **Conversational AI**: Cohere provides tools and models for building conversational interfaces, including chatbots, voice assistants, and other interactive systems.\n",
    "4. **NLP Toolkit**: Cohere offers a range of NLP tools and libraries for tasks such as tokenization, entity recognition, and sentiment analysis.\n",
    "5. **API and SDK**: Cohere provides APIs and SDKs for easy integration with other applications and services.\n",
    "\n",
    "Cohere's technology is based on advanced NLP and NLG techniques, including:\n",
    "\n",
    "1. **Transformer-based models**: Cohere's models are built using transformer-based architectures, which have been shown to be highly effective in NLP tasks.\n",
    "2. **Generative models**: Cohere's platform uses generative models, such as GANs and VAEs, to generate high-quality and coherent text.\n",
    "3. **Reinforcement learning**: Cohere's models are trained using reinforcement learning techniques, which allow them to learn from feedback and improve over time.\n",
    "\n",
    "Cohere's platform has a range of applications, including:\n",
    "\n",
    "1. **Chatbots and virtual assistants**: Cohere's technology can be used to build more intelligent and conversational chatbots and virtual assistants.\n",
    "2. **Content generation**: Cohere's platform can be used to generate high-quality content, such as articles, social media posts, and product descriptions.\n",
    "3. **Language translation**: Cohere's technology can be used to improve language translation and enable more accurate and fluent translations.\n",
    "4. **Customer service**: Cohere's platform can be used to build more intelligent and responsive customer service systems.\n",
    "\n",
    "Overall, Cohere is a powerful platform that enables developers to build more intelligent and human-like conversational interfaces, and to generate high-quality and engaging text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e--hMIfWIwsj"
   },
   "source": [
    "# Comparing and Evaluating LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1716063135036,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "dNA4TsHpu6OM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your openai  api  key here\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"your api key here\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your_token_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6931,
     "status": "ok",
     "timestamp": 1716063146071,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "J-KFB7J_u_3L",
    "outputId": "6aaa7370-47d8-46c2-b46a-85332d82d1ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.2.0\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjtcjrq7PnAb"
   },
   "source": [
    "## Setting Up the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1716063668615,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "3arHx18MQqIb"
   },
   "outputs": [],
   "source": [
    "overal_temperature = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqwsGJDhvAQ5"
   },
   "source": [
    "#### Setting up Flan models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9344,
     "status": "ok",
     "timestamp": 1716063838725,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "HM5hTtP5Xzqp",
    "outputId": "d2e60c1c-ebd9-439e-d787-4025e65e277a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.6)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.0)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.59)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1716063807796,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "lgesD0jrvDyG",
    "outputId": "1366ff08-15c8-4d19-9e85-0d0d554049d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "\n",
    "flan_20B = HuggingFaceHub(repo_id=\"google/flan-ul2\",\n",
    "                         model_kwargs={\"temperature\":overal_temperature,\n",
    "                                       \"max_new_tokens\":200}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716063838725,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "ys9FQLsISSCK"
   },
   "outputs": [],
   "source": [
    "flan_t5xxl = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\",\n",
    "                         model_kwargs={\"temperature\":overal_temperature,\n",
    "                                       \"max_new_tokens\":200}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716063845564,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "02EPvATsQytC"
   },
   "outputs": [],
   "source": [
    "# unfortunately not working\n",
    "# GPTNeoXT_20B = HuggingFaceHub(repo_id=\"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\n",
    "#                          model_kwargs={\"temperature\":overal_temperature,\n",
    "#                                        \"max_new_tokens\":200}\n",
    "#                          ) bigscience/bloom-7b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716063847940,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "HgVv5srjXZOK"
   },
   "outputs": [],
   "source": [
    "# unfortunately not working\n",
    "# bloom7B = HuggingFaceHub(repo_id=\"bigscience/bloom-7b1\",\n",
    "#                          model_kwargs={\"temperature\":overal_temperature,\n",
    "#                                        \"max_new_tokens\":200}\n",
    "#                          )\n",
    "\n",
    "gpt_j6B = HuggingFaceHub(repo_id=\"EleutherAI/gpt-j-6B\",\n",
    "                         model_kwargs={\"temperature\":overal_temperature,\n",
    "                                       \"max_new_tokens\":100}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6yiwXNnvzxO"
   },
   "source": [
    "#### Setting up OpenAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1716064457150,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "-lzO5PfUpwfv",
    "outputId": "f37eedde-8983-40dd-9384-92275739a8d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "\n",
    "chatGPT_turbo = OpenAIChat(model_name='gpt-3.5-turbo',\n",
    "             temperature=overal_temperature,\n",
    "             max_tokens = 256,\n",
    "             )\n",
    "\n",
    "# gpt3_davinici_003 = OpenAI(model_name='text-davinci-003',  # text-davinci-003 has been deprecated\n",
    " #            temperature=overal_temperature,\n",
    " #            max_tokens = 256,\n",
    " #           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKiXoHdvTDmc"
   },
   "source": [
    "#### Setting up Cohere models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1716064467309,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Ca3oLfQPTIDV"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1716064465904,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "guizikdlTIDX"
   },
   "outputs": [],
   "source": [
    "cohere_command_xl = Cohere(model='command-xlarge',\n",
    "             temperature=0.1,\n",
    "             max_tokens = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1716064469955,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "hWHCDu8dTDmd"
   },
   "outputs": [],
   "source": [
    "cohere_command_xl_nightly = Cohere(model='command-xlarge-nightly',\n",
    "             temperature=0.1,\n",
    "             max_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDWW8nDERcGU"
   },
   "source": [
    "## Set up a comparison lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716064471119,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "sy4s37W9m7X6"
   },
   "outputs": [],
   "source": [
    "from langchain.model_laboratory import ModelLaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716064473178,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "zZUMGKuvn_HV"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1716064646874,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "9s07AIuJm_Gv"
   },
   "outputs": [],
   "source": [
    "lab = ModelLaboratory.from_llms([\n",
    "                                 chatGPT_turbo,\n",
    "                                 gpt3_davinici_003,\n",
    "                                 gpt_j6B,\n",
    "                                 flan_20B,\n",
    "                                 flan_t5xxl,\n",
    "                                 cohere_command_xl,\n",
    "                                 cohere_command_xl_nightly\n",
    "                                 ], prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDQ8VcedrkIw"
   },
   "source": [
    "Let's run it on some and compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQnXvLSPbf4g"
   },
   "outputs": [],
   "source": [
    "lab.compare(\"What is the opposite of up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKgOCOoEbZSH"
   },
   "source": [
    "Input:\n",
    "What is the opposite of up?\n",
    "\n",
    "OpenAIChat\n",
    "Params: {'model_name': 'gpt-3.5-turbo', 'temperature': 0.1, 'max_tokens': 256}\n",
    "\n",
    "\n",
    "Up refers to a direction that is higher or above a certain point. The opposite of up would be a direction that is lower or below that same point. Therefore, the opposite of up is down.\n",
    "\n",
    "OpenAI\n",
    "Params: {'model_name': 'text-davinci-003', 'temperature': 0.1, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1, 'request_timeout': None, 'logit_bias': {}}\n",
    " The opposite of up is down.\n",
    "\n",
    "HuggingFaceHub\n",
    "Params: {'repo_id': 'EleutherAI/gpt-j-6B', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100}}\n",
    "\n",
    "\n",
    "Step 1: What is the opposite of up?\n",
    "\n",
    "The opposite of up is down.\n",
    "\n",
    "Step 2: What is the opposite of down?\n",
    "\n",
    "The opposite of down is up.\n",
    "\n",
    "Step 3: What is the opposite of up?\n",
    "\n",
    "The opposite of up is down.\n",
    "\n",
    "Step 4: What is the opposite of down?\n",
    "\n",
    "The opposite of down is up.\n",
    "\n",
    "Step 5: What is the opposite of up?\n",
    "\n",
    "The opposite\n",
    "\n",
    "HuggingFaceHub\n",
    "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
    "Down is the opposite of up. Up is the action of rising. Down is the action of sinking. The answer: down.\n",
    "\n",
    "HuggingFaceHub\n",
    "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
    "Down is the opposite of up. Down is the direction of the earth. Down is the direction of the sun. The sun is always in the sky. The sun is always in the sky. The answer: down.\n",
    "\n",
    "Cohere\n",
    "Params: {'model': 'command-xlarge', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
    " If you are up, you are above something. If you are below something, you are down. So the opposite of up is down.\n",
    "\n",
    "Cohere\n",
    "Params: {'model': 'command-xlarge-nightly', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
    " The opposite of up is not down. The opposite of up is not over. The opposite of up is not around. The opposite of up is not away. The opposite of up is not back. The opposite of up is not down. The opposite of up is not under. The opposite of up is not in. The opposite of up is not out. The opposite of up is not over. The opposite of up is not around. The opposite of up is not away. The opposite of up is not back. The opposite of up is not down. The opposite of up is not under. The opposite of up is not in. The opposite of up is not out. The opposite of up is not over. The opposite of up is not around. The opposite of up is not away. The opposite of up is not back. The opposite of up is not down. The opposite of up is not under. The opposite of up is not in. The opposite of up is not out. The opposite of up is not over. The opposite of up is not around. The opposite of up is not away. The opposite of up is not back. The opposite of up is not down. The opposite of up is not under. The opposite of up is not in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26064,
     "status": "ok",
     "timestamp": 1678785947438,
     "user": {
      "displayName": "Sam Witteveen",
      "userId": "13451642680591748340"
     },
     "user_tz": -480
    },
    "id": "puWRd2nwT5eD",
    "outputId": "bd65d7ea-ddcd-41b2-e6a4-9b162ed704a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "1. The cafeteria had 23 apples.\n",
      "2. They used 20 for lunch, which means they have 23 - 20 = 3 apples left.\n",
      "3. They bought 6 more apples, which means they now have 3 + 6 = 9 apples. \n",
      "\n",
      "Therefore, the cafeteria now has 9 apples.\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.1, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1, 'request_timeout': None, 'logit_bias': {}}\n",
      "\u001b[33;1m\u001b[1;3m \n",
      "\n",
      "Step 1: The cafeteria had 23 apples. \n",
      "\n",
      "Step 2: They used 20 for lunch. \n",
      "\n",
      "Step 3: They bought 6 more. \n",
      "\n",
      "Step 4: So, they have 23 + 6 = 29 apples.\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'EleutherAI/gpt-j-6B', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100}}\n",
      "\u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "Step 1: Let's say they bought 6 more apples.\n",
      "\n",
      "Step 2: Let's say they used 20 for lunch.\n",
      "\n",
      "Step 3: Let's say they bought 6 more apples.\n",
      "\n",
      "Step 4: Let's say they used 20 for lunch.\n",
      "\n",
      "Step 5: Let's say they bought 6 more apples.\n",
      "\n",
      "Step 6: Let's say they used 20 for lunch.\n",
      "\n",
      "Step 7: Let's say they bought 6 more apples.\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[32;1m\u001b[1;3mThey had 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. So the answer is 9.\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[31;1m\u001b[1;3mThe cafeteria has 23 - 20 = 3 apples left. They bought 6 + 3 = 7 apples. The cafeteria has 7 - 3 = 2 apples.\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "They had 23 apples.\n",
      "\n",
      "They used 20 for lunch.\n",
      "\n",
      "They bought 6 more.\n",
      "\n",
      "So, they must have 7 apples left.\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge-nightly', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "  1. The cafeteria had 23 apples.\n",
      "  2. They used 20 for lunch.\n",
      "  3. They bought 6 more.\n",
      "  4. How many apple do they have?\n",
      "\n",
      "Let's put the information in order.\n",
      "\n",
      "  1. The cafeteria had 23 apples.\n",
      "  2. They used 20 for lunch.\n",
      "  3. They bought 6 more.\n",
      "  4. How many apple do they have?\n",
      "\n",
      "Now we can answer the question. They have 23 apples.\n",
      "\n",
      "Question: Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
      "\n",
      "Answer: Let's think step by step.\n",
      "\n",
      "  1. The cafeteria had 23 apples.\n",
      "  2. They used 20 for lunch.\n",
      "  3. They bought 6 more.\n",
      "  4. How many apple do they have?\n",
      "\n",
      "Let's put the information in order.\n",
      "\n",
      "  1. The cafeteria had 23 apples.\n",
      "  2. They used 20 for lunch.\n",
      "  3. They bought 6 more.\n",
      "  4. How many apple do they\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare(\"Answer the following question by reasoning step by step. The cafeteria had 23 apples. \\\n",
    "If they used 20 for lunch, and bought 6 more, how many apple do they have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34823,
     "status": "ok",
     "timestamp": 1678786005107,
     "user": {
      "displayName": "Sam Witteveen",
      "userId": "13451642680591748340"
     },
     "user_tz": -480
    },
    "id": "AP63DmDbaY_X",
    "outputId": "c1d92265-aeeb-48d7-f3ec-16d4a186a202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "\n",
      "Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.\n",
      "\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "First, we need to establish that Geoffrey Hinton is a real person who is alive today, while George Washington was a historical figure who died in 1799. Therefore, it is impossible for them to have a conversation in the traditional sense.\n",
      "\n",
      "However, if we were to imagine a hypothetical scenario where time travel was possible, and Geoffrey Hinton could travel back in time to meet George Washington, there would still be significant barriers to having a meaningful conversation.\n",
      "\n",
      "Firstly, George Washington lived in a very different time period with different cultural norms, language, and technology. It is likely that he would struggle to understand many of the concepts and ideas that Geoffrey Hinton would want to discuss.\n",
      "\n",
      "Secondly, Geoffrey Hinton is a computer scientist and artificial intelligence researcher, while George Washington was a military leader and politician. They would have very different areas of expertise and interests, making it difficult to find common ground for a conversation.\n",
      "\n",
      "Therefore, while it is technically possible for Geoffrey Hinton to have a conversation with George Washington in a hypothetical scenario, it is unlikely that it would be a productive or meaningful exchange.\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.1, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1, 'request_timeout': None, 'logit_bias': {}}\n",
      "\u001b[33;1m\u001b[1;3m First, Geoffrey Hinton is a living person and George Washington is a deceased person. Second, a conversation requires two living people to communicate with each other. Therefore, no, Geoffrey Hinton cannot have a conversation with George Washington.\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'EleutherAI/gpt-j-6B', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100}}\n",
      "\u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "1.  George Washington is dead.\n",
      "\n",
      "2.  Geoffrey Hinton is not dead.\n",
      "\n",
      "3.  Geoffrey Hinton is not a ghost.\n",
      "\n",
      "4.  Geoffrey Hinton is not a zombie.\n",
      "\n",
      "5.  Geoffrey Hinton is not a vampire.\n",
      "\n",
      "6.  Geoffrey Hinton is not a werewolf.\n",
      "\n",
      "7.  Geoffrey Hinton is not a ghost.\n",
      "\n",
      "8.  Geoffrey Hinton is not a zombie\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[32;1m\u001b[1;3mGeorge Washington died in 1799. Geoffrey Hinton was born in 1959. So, the final answer is no.\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[31;1m\u001b[1;3mGeorge Washington died in 1799. Geoffrey Hinton was born in 1939. The answer: no.\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Is Hinton a real person? Yes.\n",
      "Is Washington a real person? Yes.\n",
      "Are they both alive? No.\n",
      "Are they both dead? No.\n",
      "Do they live in the same time period? No.\n",
      "Do they live in the same country? No.\n",
      "Do they live on the same planet? Yes.\n",
      "Can they have a conversation over the phone? Yes.\n",
      "\n",
      "So the answer is: Yes, they could have a conversation over the phone.\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge-nightly', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Step 1: Is Geoffrey Hinton a real person?\n",
      "\n",
      "Yes.\n",
      "\n",
      "Step 2: Is George Washington a real person?\n",
      "\n",
      "Yes.\n",
      "\n",
      "Step 3: Are they both alive?\n",
      "\n",
      "No.\n",
      "\n",
      "Step 4: Can they both speak?\n",
      "\n",
      "Yes.\n",
      "\n",
      "Step 5: Can they both understand spoken language?\n",
      "\n",
      "Yes.\n",
      "\n",
      "Step 6: Can they both speak the same language?\n",
      "\n",
      "No.\n",
      "\n",
      "Step 7: Can they both understand the same language?\n",
      "\n",
      "No.\n",
      "\n",
      "Conclusion: They cannot have a conversation.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare('''\n",
    "Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1716065350568,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Uoq5C6eMT5jr"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"You are a creative story teller who can write wonderful interesting short stories: {question}\n",
    "\n",
    "Story:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "lab = ModelLaboratory.from_llms([\n",
    "                                 chatGPT_turbo,\n",
    "                                 gpt3_davinici_003,\n",
    "                                 gpt_j6B,\n",
    "                                 flan_20B,\n",
    "                                 flan_t5xxl,\n",
    "                                 cohere_command_xl,\n",
    "                                 cohere_command_xl_nightly\n",
    "                                 ], prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67136,
     "status": "ok",
     "timestamp": 1678786095751,
     "user": {
      "displayName": "Sam Witteveen",
      "userId": "13451642680591748340"
     },
     "user_tz": -480
    },
    "id": "lo1mo_8OT5nI",
    "outputId": "37e23fe7-85a0-4eaf-e184-5d69ac711cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Write a sad story about carrot named Jason. The story should start with the carrot being a professional athlete of some kind, and end with the carrot having his heart broken.\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[36;1m\u001b[1;3mJason was a carrot like no other. He was a professional athlete, a runner to be exact. He had won numerous races and had a bright future ahead of him. He was the pride of his family and the envy of his peers.\n",
      "\n",
      "Jason had always been passionate about running. He loved the feeling of the wind in his leaves and the adrenaline rush that came with every race. He trained hard every day, pushing himself to the limit, always striving to be better.\n",
      "\n",
      "One day, Jason met a beautiful tomato named Sarah. She was a fellow athlete, a swimmer. They hit it off immediately and soon became inseparable. They trained together, ate together, and even slept together.\n",
      "\n",
      "Jason was head over heels in love with Sarah. He had never felt this way before. He knew that she was the one for him, and he was determined to make her his forever.\n",
      "\n",
      "But fate had other plans. One day, Sarah was diagnosed with a rare disease that left her unable to swim. She was devastated, and so was Jason. He tried to be there for her, to support her, but it was too much for him to bear.\n",
      "\n",
      "As Sarah's condition worsened, Jason's heart broke. He could no longer focus on his running, and his performance suffered\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.1, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1, 'request_timeout': None, 'logit_bias': {}}\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Jason the carrot was a professional athlete. He was the fastest runner in the vegetable kingdom, and he was proud of his accomplishments. He was admired by all the other vegetables, and he was always the life of the party.\n",
      "\n",
      "One day, Jason met a beautiful carrot named Daisy. She was the most beautiful carrot he had ever seen, and he was instantly smitten. He asked her out on a date, and she said yes.\n",
      "\n",
      "They went on many dates, and Jason was sure that Daisy was the one for him. He was so in love with her that he decided to propose. He bought her a beautiful diamond ring and asked her to marry him.\n",
      "\n",
      "To his surprise, Daisy said no. She told him that she wasn't ready for marriage yet, and that she needed more time to think about it. Jason was heartbroken. He had never felt so much pain before.\n",
      "\n",
      "He tried to move on, but he couldn't. He was so in love with Daisy that he couldn't bear to be without her. He stopped running and stopped competing in races. He was too sad to do anything.\n",
      "\n",
      "He eventually moved away, hoping that the distance would help him forget about Daisy. But no matter how far he\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'EleutherAI/gpt-j-6B', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100}}\n",
      "\u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "Jason was a professional athlete. He was a great athlete. He was the best athlete in the world. He was the best athlete in the world. He was the best athlete in the world. He was the best athlete in the world. He was the best athlete in the world. He was the best athlete in the world. He was the best athlete in the world. He was the best athlete in the world. He was the best athlete in the world. He was the best athlete\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[32;1m\u001b[1;3mJason was a professional carrot. He was a great athlete. He was a great basketball player. He was a great football player. He was a great baseball player. He was a great swimmer. He was a great runner. He was a great skateboarder. He was a great gymnast. He was a great dancer. He was a great singer. He was a great actor. He was a great comedian. He was a great teacher. He was a great doctor. He was a great lawyer. He was a great politician. He was a great businessman. He was a great teacher. He was a great singer. He was a great dancer. He was a great actor. He was a great comedian. He was a great businessman. He was a great teacher. He was a great doctor. He was a great lawyer.\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[31;1m\u001b[1;3mJason was a professional athlete. He was a carrot. He was a very good carrot. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was a very good athlete. He was \u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Jason was a professional athlete, a runner to be exact. He was the best in his field, and he had a lot of fans. But one day, he met a girl named Jessica. She was a professional athlete too, and she was the best in her field. They fell in love and got married.\n",
      "\n",
      "But then, one day, Jessica cheated on Jason. She had an affair with his best friend. Jason was heartbroken. He didn't know what to do. He didn't want to live anymore. So he quit his job and moved to a small town in the middle of nowhere.\n",
      "\n",
      "He started working at a farm, growing carrots. He was good at it, and he made a lot of friends. But he was still heartbroken.\n",
      "\n",
      "One day, he met a girl named Emily. She was a professional athlete too, and she was the best in her field. They fell in love and got married.\n",
      "\n",
      "But then, one day, Emily cheated on Jason. She had an affair with his best friend. Jason was heartbroken. He didn't know what to do. He didn't want to live anymore. So he quit his job and moved to a small town in the middle of nowhere.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge-nightly', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Jason was a professional athlete. He was a carrot. And he was in love.\n",
      "\n",
      "Jason was a carrot, but he was no ordinary carrot. He was a professional athlete. He was a track and field carrot. He was the fastest carrot in the world.\n",
      "\n",
      "Jason was in love with a beautiful carrot named Jessica. She was a professional athlete too. She was a track and field carrot as well.\n",
      "\n",
      "Jason and Jessica met at a track and field competition. They fell in love at first sight. They both knew that they were meant to be together.\n",
      "\n",
      "Jason and Jessica were the perfect couple. They were always together. They did everything together. They were always happy.\n",
      "\n",
      "But one day, Jason's heart was broken. Jessica broke up with him. She said that she needed some space.\n",
      "\n",
      "Jason was heartbroken. He didn't know what to do. He didn't know how to move on.\n",
      "\n",
      "So, Jason decided to quit track and field. He quit his job as a professional athlete. He moved to a small town and started working at a carrot farm.\n",
      "\n",
      "Jason was happy at the carrot farm. He had a new life. He had new\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare('''Write a sad story about carrot named Jason. The story should \\\n",
    "start with the carrot being a professional athlete of some kind, \\\n",
    "and end with the carrot having his heart broken.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716065650215,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "c2c6EByDT5rQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Answer the question to the best of your abilities but if you are not sure then answer you don't know: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "lab = ModelLaboratory.from_llms([\n",
    "                                 chatGPT_turbo,\n",
    "                                 gpt3_davinici_003,\n",
    "                                 gpt_j6B,\n",
    "                                 flan_20B,\n",
    "                                 flan_t5xxl,\n",
    "                                 cohere_command_xl,\n",
    "                                 cohere_command_xl_nightly\n",
    "                                 ], prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42280,
     "status": "ok",
     "timestamp": 1678786257097,
     "user": {
      "displayName": "Sam Witteveen",
      "userId": "13451642680591748340"
     },
     "user_tz": -480
    },
    "id": "7qCuIkZST5uK",
    "outputId": "a61a46ae-b5da-456b-deb7-6641d77716fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[36;1m\u001b[1;3mYou are likely on a stationary bicycle or trainer.\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.1, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1, 'request_timeout': None, 'logit_bias': {}}\n",
      "\u001b[33;1m\u001b[1;3m You are not moving because you are coasting, meaning you are not pedaling and the bike is still in motion due to the momentum from your previous pedaling.\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'EleutherAI/gpt-j-6B', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100}}\n",
      "\u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "The bicycle is moving because the bicycle is moving. The bicycle is moving because the bicycle is moving. The bicycle is moving because the bicycle is moving. The bicycle is moving because the bicycle is moving. The bicycle is moving because the bicycle is moving. The bicycle is moving because the bicycle is moving. The bicycle is moving because the bicycle is moving. The bicycle is moving because the bicycle is moving. The bicycle is moving because the bicycle is moving. The bicycle is moving because the bicycle is\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[32;1m\u001b[1;3mI am stationary\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[31;1m\u001b[1;3mI am looking at the wrong angle.\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "The pedals are moving fast. I look into the mirror and I am not moving. Why is this?\n",
      "The answer is: I am not moving because I am on a moving bicycle.\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge-nightly', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[33;1m\u001b[1;3m The mirror is on a stationary object.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare('''I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOks9w5ndiqu"
   },
   "source": [
    "### Fact Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716065776687,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "JydfVsh8eAFx"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "lab = ModelLaboratory.from_llms([\n",
    "                                 chatGPT_turbo,\n",
    "                                 gpt3_davinici_003,\n",
    "                                 gpt_j6B,\n",
    "                                 flan_20B,\n",
    "                                 flan_t5xxl,\n",
    "                                 cohere_command_xl,\n",
    "                                 cohere_command_xl_nightly\n",
    "                                 ], prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13853,
     "status": "ok",
     "timestamp": 1678786488123,
     "user": {
      "displayName": "Sam Witteveen",
      "userId": "13451642680591748340"
     },
     "user_tz": -480
    },
    "id": "crsqwwmrdLQl",
    "outputId": "69a057a4-f6b7-4a9d-c416-26dd9b563bc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Please answer the question:\n",
      "\n",
      "Who is the OnePlus COO?\n",
      "\n",
      "\n",
      "Output in the format: [first_name, surname]\n",
      "\n",
      "\n",
      "\n",
      "Smartphone makers searched for a way forward at MWC 2023\n",
      "Foldables, 6G, light shows -- there are a lot of ideas floating around, but no one has cracked the code\n",
      "The slowdown was inevitable, of course. Nothing stays hot forever — especially in this industry. By tech standards, smartphones have had a good run, but the last few years have seen device makers searching for the magic bullet to help the sales slide reverse course. The arrival of 5G was a nice reprieve, but next-generation telecom standards don’t arrive every year.\n",
      "\n",
      "“I personally think foldables are supply chain-driven innovation and not consumer insights,” Pei said. “Somebody invents OLED, and they can make a lot of money, because it’s a great technology. Then after a few years, a lot more companies make that, so they need to lower their prices. So they need to figure out what else they can sell at a higher margin. They develop flexible OLEDs, which they can sell at a higher price.”\n",
      "It’s hard not to be cynical about this stuff sometimes. Ditto for concept devices, though as I noted in my “ode to weird tech” post, as someone who follows this stuff for a living, I’m a fan of weirdness for weirdness sake, be it the rollable Motorola Rizr screen or the OnePlus glowing cooling fluid. Certainly following the automotive industry’s lead of creating concept devices is a trend that is likely to only become more pervasive.\n",
      "\n",
      "OnePlus COO Kinder Liu told me this week that gauging consumer interest is one of the “multiple reasons” his company is engaging with the concept. He added, “Also, we want to encourage continuous innovation inside our company.”\n",
      "\n",
      "Pretty much everyone I engaged with this week echoed the sentiment that smartphones are in a rut. For the first time, however, it’s not a foregone conclusion that there’s a way of getting out.\n",
      "\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[36;1m\u001b[1;3m[Kinder, Liu]\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.1, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1, 'request_timeout': None, 'logit_bias': {}}\n",
      "\u001b[33;1m\u001b[1;3m [Kinder, Liu]\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'EleutherAI/gpt-j-6B', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100}}\n",
      "\u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "OnePlus COO Kinder Liu\n",
      "\n",
      "Smartphone makers searched for a way forward at MWC 2023\n",
      "\n",
      "Foldables, 6G, light shows -- there are a lot of ideas floating around, but no one has cracked the code\n",
      "\n",
      "The slowdown was inevitable, of course. Nothing stays hot forever — especially in this industry. By tech standards, smartphones have had a good run, but the last few years have seen device makers searching for the magic bullet to help the sales\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[32;1m\u001b[1;3mKinder [surname] Liu\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[31;1m\u001b[1;3mKinder, Liu\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Kinder Liu\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge-nightly', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Kinder Liu\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare('''Please answer the question:\\n\n",
    "Who is the OnePlus COO?\\n\\n\n",
    "Output in the format: [first_name, surname]\\n\\n\n",
    "\n",
    "Smartphone makers searched for a way forward at MWC 2023\n",
    "Foldables, 6G, light shows -- there are a lot of ideas floating around, but no one has cracked the code\n",
    "The slowdown was inevitable, of course. Nothing stays hot forever — especially in this industry. By tech standards, smartphones have had a good run, but the last few years have seen device makers searching for the magic bullet to help the sales slide reverse course. The arrival of 5G was a nice reprieve, but next-generation telecom standards don’t arrive every year.\n",
    "\n",
    "“I personally think foldables are supply chain-driven innovation and not consumer insights,” Pei said. “Somebody invents OLED, and they can make a lot of money, because it’s a great technology. Then after a few years, a lot more companies make that, so they need to lower their prices. So they need to figure out what else they can sell at a higher margin. They develop flexible OLEDs, which they can sell at a higher price.”\n",
    "It’s hard not to be cynical about this stuff sometimes. Ditto for concept devices, though as I noted in my “ode to weird tech” post, as someone who follows this stuff for a living, I’m a fan of weirdness for weirdness sake, be it the rollable Motorola Rizr screen or the OnePlus glowing cooling fluid. Certainly following the automotive industry’s lead of creating concept devices is a trend that is likely to only become more pervasive.\n",
    "\n",
    "OnePlus COO Kinder Liu told me this week that gauging consumer interest is one of the “multiple reasons” his company is engaging with the concept. He added, “Also, we want to encourage continuous innovation inside our company.”\n",
    "\n",
    "Pretty much everyone I engaged with this week echoed the sentiment that smartphones are in a rut. For the first time, however, it’s not a foregone conclusion that there’s a way of getting out.\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19952,
     "status": "ok",
     "timestamp": 1678786622924,
     "user": {
      "displayName": "Sam Witteveen",
      "userId": "13451642680591748340"
     },
     "user_tz": -480
    },
    "id": "H3QT55S3eK2y",
    "outputId": "5f29a5ae-8cc4-4906-c53b-b7cc4559f259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Please answer the question:\n",
      "\n",
      "What is a supply chain driven innovation?\n",
      "\n",
      "\n",
      "\n",
      "Smartphone makers searched for a way forward at MWC 2023\n",
      "Foldables, 6G, light shows -- there are a lot of ideas floating around, but no one has cracked the code\n",
      "The slowdown was inevitable, of course. Nothing stays hot forever — especially in this industry. By tech standards, smartphones have had a good run, but the last few years have seen device makers searching for the magic bullet to help the sales slide reverse course. The arrival of 5G was a nice reprieve, but next-generation telecom standards don’t arrive every year.\n",
      "\n",
      "“I personally think foldables are supply chain-driven innovation and not consumer insights,” Pei said. “Somebody invents OLED, and they can make a lot of money, because it’s a great technology. Then after a few years, a lot more companies make that, so they need to lower their prices. So they need to figure out what else they can sell at a higher margin. They develop flexible OLEDs, which they can sell at a higher price.”\n",
      "It’s hard not to be cynical about this stuff sometimes. Ditto for concept devices, though as I noted in my “ode to weird tech” post, as someone who follows this stuff for a living, I’m a fan of weirdness for weirdness sake, be it the rollable Motorola Rizr screen or the OnePlus glowing cooling fluid. Certainly following the automotive industry’s lead of creating concept devices is a trend that is likely to only become more pervasive.\n",
      "\n",
      "OnePlus COO Kinder Liu told me this week that gauging consumer interest is one of the “multiple reasons” his company is engaging with the concept. He added, “Also, we want to encourage continuous innovation inside our company.”\n",
      "\n",
      "Pretty much everyone I engaged with this week echoed the sentiment that smartphones are in a rut. For the first time, however, it’s not a foregone conclusion that there’s a way of getting out.\n",
      "\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[36;1m\u001b[1;3mA supply chain-driven innovation is when a company develops a new product or technology based on the availability and cost of materials and components in their supply chain, rather than consumer demand or insights. This can lead to the development of new products with higher profit margins, such as flexible OLED screens in smartphones.\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.1, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1, 'request_timeout': None, 'logit_bias': {}}\n",
      "\u001b[33;1m\u001b[1;3m Supply chain driven innovation is when companies use new technologies and materials to create products that can be sold at a higher margin. This is often done in response to market forces, such as the need to lower prices due to increased competition. Examples of this include the development of flexible OLEDs and concept devices, such as the rollable Motorola Rizr screen or the OnePlus glowing cooling fluid.\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'EleutherAI/gpt-j-6B', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100}}\n",
      "\u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "Smartphone makers searched for a way forward at MWC 2023\n",
      "\n",
      "Foldables, 6G, light shows -- there are a lot of ideas floating around, but no one has cracked the code\n",
      "\n",
      "The slowdown was inevitable, of course. Nothing stays hot forever — especially in this industry. By tech standards, smartphones have had a good run, but the last few years have seen device makers searching for the magic bullet to help the sales slide reverse course. The arrival of 5\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[32;1m\u001b[1;3mfoldables\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[31;1m\u001b[1;3mfoldables\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Supply chain driven innovation\u001b[0m\n",
      "\n",
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': 'command-xlarge-nightly', 'max_tokens': 256, 'temperature': 0.1, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Supply chain driven innovation is when a new technology is created because of the need to sell a new product at a higher margin.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare('''Please answer the question:\\n\n",
    "What is a supply chain driven innovation?\\n\\n\n",
    "\n",
    "Smartphone makers searched for a way forward at MWC 2023\n",
    "Foldables, 6G, light shows -- there are a lot of ideas floating around, but no one has cracked the code\n",
    "The slowdown was inevitable, of course. Nothing stays hot forever — especially in this industry. By tech standards, smartphones have had a good run, but the last few years have seen device makers searching for the magic bullet to help the sales slide reverse course. The arrival of 5G was a nice reprieve, but next-generation telecom standards don’t arrive every year.\n",
    "\n",
    "“I personally think foldables are supply chain-driven innovation and not consumer insights,” Pei said. “Somebody invents OLED, and they can make a lot of money, because it’s a great technology. Then after a few years, a lot more companies make that, so they need to lower their prices. So they need to figure out what else they can sell at a higher margin. They develop flexible OLEDs, which they can sell at a higher price.”\n",
    "It’s hard not to be cynical about this stuff sometimes. Ditto for concept devices, though as I noted in my “ode to weird tech” post, as someone who follows this stuff for a living, I’m a fan of weirdness for weirdness sake, be it the rollable Motorola Rizr screen or the OnePlus glowing cooling fluid. Certainly following the automotive industry’s lead of creating concept devices is a trend that is likely to only become more pervasive.\n",
    "\n",
    "OnePlus COO Kinder Liu told me this week that gauging consumer interest is one of the “multiple reasons” his company is engaging with the concept. He added, “Also, we want to encourage continuous innovation inside our company.”\n",
    "\n",
    "Pretty much everyone I engaged with this week echoed the sentiment that smartphones are in a rut. For the first time, however, it’s not a foregone conclusion that there’s a way of getting out.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9KyaUh0edBY"
   },
   "source": [
    "# **Summary of Models Comparison**\n",
    "\n",
    "# **ChatGPT_Turbo**\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Fine-tuned on a massive dataset of human-generated text, making it highly conversational\n",
    "* Excellent at understanding context and responding accordingly\n",
    "* Can generate human-like text in various styles and formats\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* May not be as knowledgeable as other models in specific domains\n",
    "* Can be prone to generating overly simplistic or generic responses\n",
    "\n",
    "# **GPT-3 Davinci-003**\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* One of the most advanced language models, with an enormous parameter count (175 billion)\n",
    "* Can generate highly coherent and informative text on a wide range of topics\n",
    "* Excellent at tasks like text classification, sentiment analysis, and language translation\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Requires significant computational resources and memory\n",
    "* Can be slow to respond due to its massive size\n",
    "* May generate overly verbose or repetitive text\n",
    "\n",
    "# **GPT-J-6B**\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* A smaller and more efficient version of GPT-3, with 6 billion parameters\n",
    "* Still capable of generating high-quality text, but with faster response times\n",
    "* More accessible to developers due to its smaller size\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* May not be as knowledgeable as GPT-3 in certain domains\n",
    "* Can struggle with very long-range dependencies or complex tasks\n",
    "\n",
    "#  **FLAN-20B**\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* A highly advanced language model with 20 billion parameters\n",
    "* Trained on a massive dataset of text from the internet\n",
    "* Excellent at tasks like conversational dialogue, text classification, and language translation\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Requires significant computational resources and memory\n",
    "* May generate overly verbose or repetitive text\n",
    "* Can be slow to respond due to its massive size\n",
    "\n",
    "# **FLAN-T5-XXL**\n",
    "\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* A variant of FLAN-20B, with a focus on text-to-text generation tasks\n",
    "* Excellent at tasks like text summarization, question answering, and dialogue generation\n",
    "* Can generate highly coherent and informative text\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* May not be as knowledgeable as FLAN-20B in certain domains\n",
    "* Can struggle with very long-range dependencies or complex tasks\n",
    "\n",
    "# **Cohere Command-XL**\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* A highly advanced language model with a focus on conversational AI\n",
    "* Excellent at tasks like dialogue generation, text classification, and language translation\n",
    "* Can generate highly coherent and informative text\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* May require significant computational resources and memory\n",
    "* Can be slow to respond due to its massive size\n",
    "* May generate overly verbose or repetitive text\n",
    "\n",
    "# **Cohere Command-XL Nightly**\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* A bleeding-edge version of Cohere Command-XL, with the latest updates and improvements\n",
    "* May include new features or capabilities not available in the standard version\n",
    "* Can generate highly coherent and informative text\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* May be less stable or more prone to errors due to its experimental nature\n",
    "* May require significant computational resources and memory\n",
    "* Can be slow to respond due to its massive size\n",
    "\n",
    "## **Key differences:**\n",
    "\n",
    "* **Size and complexity:** GPT-3 Davinci-003 and FLAN-20B are among the largest and most complex models, while GPT-J-6B and FLAN-T5-XXL are smaller and more efficient variants.\n",
    "* **Training data:** Models like ChatGPT_Turbo and Cohere Command-XL are fine-tuned on specific datasets or tasks, while models like GPT-3 and FLAN-20B are trained on massive datasets of general text.\n",
    "* **Task specialization:** Models like FLAN-T5-XXL and Cohere Command-XL are specialized for specific tasks like text-to-text generation or conversational AI, while models like GPT-3 and FLAN-20B are more general-purpose language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCsCk2GbjTex"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFIkXCRbjTwX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "13oByPHlSWD3ETd2rb56yzD4Wg5gtXzIs",
     "timestamp": 1716062290998
    },
    {
     "file_id": "1-D_omwEdbCkqwZdEbVr3bTUakRi_duvp",
     "timestamp": 1678884266057
    },
    {
     "file_id": "1RVj73nEVyl4CTo8SJ5gPzvAcN4XJipFD",
     "timestamp": 1678782615742
    },
    {
     "file_id": "1BT_kRFMP27lmwAoWeIhiie0VPs0oqShz",
     "timestamp": 1678780564852
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
