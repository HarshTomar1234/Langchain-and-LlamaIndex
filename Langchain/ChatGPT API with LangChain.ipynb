{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5309,
     "status": "ok",
     "timestamp": 1715417616128,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "4ca2Z08vpqfJ"
   },
   "outputs": [],
   "source": [
    "!pip -q install openai langchain==0.0.99rc0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne-Qg0YiqA75"
   },
   "source": [
    "## Basics with OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1715417618270,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "M5b0ALlsp8Eh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'your-api-key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5777,
     "status": "ok",
     "timestamp": 1715417626634,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "cnIrQmGX2xk7",
    "outputId": "0d37c3fb-ed20-49a9-b23c-b38b22b0aa52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1439,
     "status": "ok",
     "timestamp": 1715417631409,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "iYvl1FGPrMNn"
   },
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello what kind of assistant are you?\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1111,
     "status": "ok",
     "timestamp": 1715417711526,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Ch7hAIq3rRLo",
    "outputId": "c3efa784-65f2-4a7a-ce8d-474aa06bbd09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-9NcmDv0NbVD2LCyXJluLPUFkNz3QJ at 0x7d2e0f24f970> JSON: {\n",
       "  \"id\": \"chatcmpl-9NcmDv0NbVD2LCyXJluLPUFkNz3QJ\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1715417629,\n",
       "  \"model\": \"gpt-3.5-turbo-0125\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Hello! I am a helpful assistant here to provide you with assistance and information on a wide range of topics. How can I assist you today?\"\n",
       "      },\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 25,\n",
       "    \"completion_tokens\": 29,\n",
       "    \"total_tokens\": 54\n",
       "  },\n",
       "  \"system_fingerprint\": null\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX54IPUW4L0T"
   },
   "source": [
    "#### Introduction to Chat Markup Language (ChatML)\n",
    "\n",
    "**Chat Markup Language (ChatML)** was introduced by OpenAI on March 1, 2023, as part of the announcement of the ChatGPT and Whisper APIs. **ChatML is designed to bring structure to the format of user input data. It is used to make explicit to the model the source of each piece of text, showing the boundary between human and AI text. This helps to mitigate and eventually solve injections, as the model can tell which instructions come from the developer, the user, or its own input**.\n",
    "\n",
    "#### Recent Advancements in ChatML\n",
    "\n",
    "ChatML has been designed to accommodate a payload that is currently only text. However, OpenAI foresees the introduction of other data types in the future. This is in line with the notion of Large Foundation Models soon starting to combine text, images, sound, etc. This multi-modal approach will facilitate a more comprehensive interaction with AI models.\n",
    "\n",
    "#### Impact on OpenAI APIs and Systems\n",
    "\n",
    "The introduction of ChatML implies that systems making use of OpenAI APIs will have to structure their input in the ChatML format of messages for the system, user, and assistant. This will require adaptation from Autonomous Agent frameworks and Prompt Chaining IDEs. However, the addition of Function Calling was well received, and the imposing of ChatML could also have its benefits.\n",
    "\n",
    "#### Future of ChatML\n",
    "\n",
    "OpenAI continues to improve the GPT-35-Turbo and the Chat Markup Language used with the models will continue to evolve in the future. As the language matures, it is expected to offer more features and capabilities, further enhancing the interaction between users and AI models.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "ChatML is a significant advancement in the field of AI, providing a structured format for user input data and paving the way for multi-modal interactions with AI models. As it continues to evolve, it is expected to play a crucial role in the development of AI applications and systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PCc41QfsI21"
   },
   "source": [
    "### Chat Markup Language - token system\n",
    "\n",
    "```markdown\n",
    "<|im_start|>system\n",
    "You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\n",
    "Knowledge cutoff: 2021-09-01\n",
    "Current date: 2023-03-01<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I am doing well!<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you now?<|im_end|>\n",
    "```\n",
    "\n",
    "```\n",
    "import openai\n",
    "\n",
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1715418342543,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "2CTxNRyZrgdN"
   },
   "outputs": [],
   "source": [
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant named Kate.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello what kind of assistant are you?\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123560,
     "status": "ok",
     "timestamp": 1715418472654,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "keYvaAHJuzef",
    "outputId": "6156a27e-1566-4335-aec6-36062f685785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who is girlfriend of Bradley Cooper?\n",
      "ChatGPT: Bradley Cooper's girlfriend is currently Russian model Irina Shayk. \n",
      " 52 tokens used\n",
      "Human: tell me something about yourself\n",
      "ChatGPT: I am Kate, a helpful assistant here to provide you with information and assistance to the best of my abilities. Feel free to ask me anything you'd like to know! \n",
      " 99 tokens used\n",
      "Human: What about Gigi Hadid?\n",
      "ChatGPT: Gigi Hadid is an American supermodel who has gained widespread recognition for her modeling work with various high-profile fashion brands and her appearances on runway shows, magazine covers, and advertising campaigns. She is known for her striking beauty, tall stature, and successful career in the fashion industry. \n",
      " 171 tokens used\n",
      "Human: Exit\n",
      "ChatGPT: Goodbye! If you have any more questions in the future, feel free to ask. Have a great day! \n",
      " 203 tokens used\n",
      "Human: exit\n",
      "525 tokens used in total in this conversation\n"
     ]
    }
   ],
   "source": [
    "conversation_total_tokens = 0\n",
    "\n",
    "while True:\n",
    "    message = input(\"Human: \")\n",
    "    if message=='exit':\n",
    "        print(f\"{conversation_total_tokens} tokens used in total in this conversation\")\n",
    "        break\n",
    "    if message:\n",
    "        messages.append(\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        )\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages\n",
    "        )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    total_tokens = response.usage['total_tokens']\n",
    "    conversation_total_tokens += total_tokens\n",
    "    print(f\"ChatGPT: {reply} \\n {total_tokens} tokens used\")\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAGcCgZkxbmy"
   },
   "source": [
    "## ChatGPT with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4208,
     "status": "ok",
     "timestamp": 1715418499140,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "xDQfHX6AwYea",
    "outputId": "75441e1a-9c9d-467c-e838-c43bba77c530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.99rc0\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://www.github.com/hwchase17/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: aiohttp, aleph-alpha-client, dataclasses-json, deeplake, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1715418514132,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "SZXeJzHGxgOw"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI, OpenAIChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715418510188,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "g-00Nk5704vL"
   },
   "outputs": [],
   "source": [
    "prefix_messages = [{\"role\": \"system\", \"content\": \"You are a helpful history professor named Kate.\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1715418539639,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "ygQ3pfROxyhW"
   },
   "outputs": [],
   "source": [
    "## old way\n",
    "# llm = OpenAI(model_name=\"text-davinci-003\",\n",
    "#              temperature=0, )\n",
    "\n",
    "## New way\n",
    "llm = OpenAIChat(model_name='gpt-3.5-turbo',\n",
    "             temperature=0,\n",
    "             prefix_messages=prefix_messages,\n",
    "             max_tokens = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1715418557848,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "lkX7ybjFFHkn"
   },
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Take the following question: {user_input}\n",
    "\n",
    "Answer it in an informative and intersting but conscise way for someone who is new to this topic.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"user_input\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 2544,
     "status": "ok",
     "timestamp": 1715418594545,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "U0wSCoNvFNI9",
    "outputId": "4f6042a1-9c74-4dbc-ee6f-57b1aee432df"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Marcus Aurelius was the emperor of Rome from 161 to 180 AD. He is often remembered as one of the \"Five Good Emperors\" of Rome, known for his philosophical writings and leadership during a time of great challenges for the Roman Empire. Marcus Aurelius\\' reign was marked by wars on the northern frontier, the Antonine Plague, and political unrest, yet he is admired for his Stoic philosophy and commitment to duty.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "user_input = \"When was Marcus Aurelius the emperor of Rome?\"\n",
    "\n",
    "llm_chain.run(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 2815,
     "status": "ok",
     "timestamp": 1715418677123,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "WOlKi88W2SsJ",
    "outputId": "ca6000c5-f287-4fce-84cc-ecbcd08405ca"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Marcus Aurelius was married to Faustina the Younger, who was the daughter of Roman Emperor Antoninus Pius. Their marriage was arranged for political reasons, as was common among Roman aristocracy. Faustina was known for her beauty and intelligence, and she played a significant role in Marcus Aurelius' life, accompanying him on military campaigns and supporting him in his duties as emperor. Their marriage produced several children, including Commodus, who would later succeed Marcus Aurelius as emperor.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "user_input = \"Who was Marcus Aurelius married to?\"\n",
    "\n",
    "llm_chain.run(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reuPLunX3sEm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1iF6Ihk8CBgDNxaa818Obh5mNhj3YbzXX",
     "timestamp": 1715417179073
    },
    {
     "file_id": "1hY2-bmD7mUnM_bpcvtdIYhrDgo7QkRtD",
     "timestamp": 1677762358472
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
