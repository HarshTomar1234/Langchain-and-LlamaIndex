{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P9j-4urd4xF"
   },
   "source": [
    "# FastChat-T5\n",
    "\n",
    "FastChat-T5 is a 3B parameter chatbot model developed by the FastChat team, primarily Dacheng Li, Lianmin Zheng, and Hao Zhang. It is based on the Flan-T5-xl model and fine-tuned on user-shared conversations collected from ShareGPT. Here are some key points about FastChat-T5:\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Architecture**: FastChat-T5 uses an encoder-decoder transformer architecture, which allows it to autoregressively generate responses to users' inputs.\n",
    "- **Training Data**: The model was trained on 70K conversations collected from ShareGPT.com, processed in the form of question answering.\n",
    "- **Training Details**: The model was fine-tuned for 3 epochs with a max learning rate of 2e-5, warmup ratio of 0.03, and a cosine learning rate schedule.\n",
    "- **Evaluation**: The model's quality was evaluated by creating a set of 80 diverse questions and using GPT-4 to judge the model outputs.\n",
    "\n",
    "### Usage\n",
    "\n",
    "FastChat-T5 can be used for various applications, including:\n",
    "\n",
    "- **Commercial Usage**: The model is suitable for commercial usage of large language models and chatbots.\n",
    "- **Research**: It can also be used for research purposes in natural language processing, machine learning, and artificial intelligence.\n",
    "\n",
    "### Performance\n",
    "\n",
    "FastChat-T5 has been reported to perform well in various tasks, including:\n",
    "\n",
    "- **Summarizing Documents**: It has been used for summarizing documents and feeding the summaries to the model with the user's query.\n",
    "- **Instruction Following**: It has been tested for instruction following and inference, and has been found to perform better than Ada and worse than ChatGPT.\n",
    "- **Financial Work**: It has been used for financial work and has been found to be more accurate than other models like T5 and UL2.\n",
    "\n",
    "### Availability\n",
    "\n",
    "FastChat-T5 is available on the Hugging Face model hub and can be downloaded and used for various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 9643,
     "status": "ok",
     "timestamp": 1718349021046,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "RRYSu48huSUW"
   },
   "outputs": [],
   "source": [
    "!pip -q install langchain tiktoken chromadb pypdf transformers InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4887,
     "status": "ok",
     "timestamp": 1718349025926,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "J-KFB7J_u_3L",
    "outputId": "f10d48af-f6de-4776-b790-9035878021ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.2.4\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: aiohttp, async-timeout, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-community\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAdw3ADcS8Ph"
   },
   "source": [
    "## QA Retrieval No Open AI - Fastchat-T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8JED6YFS8y7"
   },
   "source": [
    "Transformers is a popular open-source library developed by Hugging Face that provides a wide range of pre-trained models and tools for natural language processing (NLP) tasks. Here is a detailed explanation of transformers and the specific classes mentioned in the query:\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Transformers is a library that provides a unified interface for various transformer-based models, including BERT, RoBERTa, XLNet, and many others. It allows users to easily load and use pre-trained models for a variety of NLP tasks such as text classification, sentiment analysis, question answering, and language translation.\n",
    "\n",
    "### AutoTokenizer\n",
    "\n",
    "`AutoTokenizer` is a class in the transformers library that automatically selects the appropriate tokenizer for a given pre-trained model. It is a generic tokenizer class that can be instantiated using the `from_pretrained` method, which loads the tokenizer based on the model name or path.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "```\n",
    "\n",
    "### AutoModelForSeq2SeqLM\n",
    "\n",
    "`AutoModelForSeq2SeqLM` is a class in the transformers library that automatically selects the appropriate model for sequence-to-sequence tasks. It is used for models with an encoder-decoder architecture, such as T5 and BART, which are commonly used for tasks like machine translation, text summarization, and question answering.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Pre-trained Models**: Transformers provides a wide range of pre-trained models that can be fine-tuned for specific tasks.\n",
    "- **Auto Classes**: The library provides auto classes like `AutoTokenizer` and `AutoModelForSeq2SeqLM` that automatically select the appropriate tokenizer or model based on the model name or path.\n",
    "- **Unified Interface**: Transformers provides a unified interface for various transformer-based models, making it easy to switch between different models and tasks.\n",
    "\n",
    "### Usage\n",
    "\n",
    "Transformers can be used for various NLP tasks, including:\n",
    "\n",
    "- **Text Classification**: Transformers provides pre-trained models like BERT and RoBERTa that can be fine-tuned for text classification tasks.\n",
    "- **Language Translation**: Models like T5 and BART can be used for machine translation tasks.\n",
    "- **Question Answering**: Models like BERT and RoBERTa can be fine-tuned for question answering tasks.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Easy to Use**: Transformers provides a simple and unified interface for various transformer-based models, making it easy to use and fine-tune pre-trained models.\n",
    "- **Wide Range of Models**: The library provides a wide range of pre-trained models that can be used for various NLP tasks.\n",
    "- **Flexibility**: Transformers allows users to easily switch between different models and tasks, making it a flexible and versatile library.\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "- **Model Selection**: Choosing the right pre-trained model for a specific task can be challenging, especially for users who are new to transformers.\n",
    "- **Fine-tuning**: Fine-tuning pre-trained models requires a good understanding of the model architecture and the task at hand.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Choose the Right Model**: Select a pre-trained model that is suitable for the task at hand.\n",
    "- **Fine-tune Carefully**: Fine-tune the pre-trained model carefully, using a suitable optimizer and learning rate.\n",
    "- **Monitor Performance**: Monitor the performance of the model during fine-tuning and adjust the hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17930,
     "status": "ok",
     "timestamp": 1718349056149,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "l6XPLPVrqEaV",
    "outputId": "6188e8c5-ebd1-4c02-fc4f-356233fd24dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace new_papers/new_papers/toolformer.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace new_papers/__MACOSX/new_papers/._toolformer.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace new_papers/new_papers/Flash-attention.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace new_papers/__MACOSX/new_papers/._Flash-attention.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace new_papers/new_papers/Augmenting LLMs Survey.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace new_papers/__MACOSX/new_papers/._Augmenting LLMs Survey.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace new_papers/new_papers/ReACT.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace new_papers/__MACOSX/new_papers/._ReACT.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace new_papers/new_papers/ALiBi.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace new_papers/__MACOSX/new_papers/._ALiBi.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://www.dropbox.com/s/zoj9rnm7oyeaivb/new_papers.zip\n",
    "!unzip -q new_papers.zip -d new_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55943,
     "status": "ok",
     "timestamp": 1718349183038,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "_38HzUkvIewb",
    "outputId": "7064c12a-1bb0-4b38-943a-9ddca14cd8f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/fastchat-t5-3b-v1.0\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lmsys/fastchat-t5-3b-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9666,
     "status": "ok",
     "timestamp": 1718349192697,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "USiQe_KKfhwX",
    "outputId": "22fbc2de-39d2-4679-af5a-12660963003b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.4)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.6)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.77)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19453,
     "status": "ok",
     "timestamp": 1718349212142,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "6G98f4fXI5kT",
    "outputId": "003e4d41-f5c1-4fb5-ace0-610e73aa1162"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AnZQpL_IZZZ"
   },
   "source": [
    "# LangChain multi-doc retriever with ChromaDB\n",
    "\n",
    "***New Points***\n",
    "- Multiple Files - PDFs\n",
    "- ChromaDB - with more meta data?\n",
    "- Source info\n",
    "- gpt-3.5-turbo API\n",
    "- HuggingFace Embeddings\n",
    "- Instuctor Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqwsGJDhvAQ5"
   },
   "source": [
    "## Setting up LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9939,
     "status": "ok",
     "timestamp": 1718349336707,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "dNA4TsHpu6OM",
    "outputId": "88daef2f-dca0-493e-f276-a54e5419aaea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.3.0+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.18.0+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.23.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.5.40)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "!pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2094,
     "status": "ok",
     "timestamp": 1718349338795,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "XHVE9uFb3Ajj"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UcQKUId3X2M"
   },
   "source": [
    "## Load multiple and process documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6542,
     "status": "ok",
     "timestamp": 1718349350414,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "PRSeXXc_3Ypj"
   },
   "outputs": [],
   "source": [
    "# Load and process the text files\n",
    "# loader = TextLoader('single_text_file.txt')\n",
    "loader = DirectoryLoader('./new_papers/new_papers/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1718349354006,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "vT6KgAIT_BtB",
    "outputId": "feb746fc-bd02-439f-c9b0-a5d42f0fcc53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1718349354007,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "3__nT0D4Fkmg"
   },
   "outputs": [],
   "source": [
    "#splitting the text into\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1718349354007,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "TlU5AlqY4gwj",
    "outputId": "c36de37b-8da3-45ca-a0a3-a2ea8298abc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1718349354007,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Bg6-9jwU4ja_",
    "outputId": "7ab02665-9586-4e7f-e2a4-2d439507bbce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='has time and memory complexity quadratic in sequence length. An important question is whether making\\nattention faster and more memory-eﬃcient can help Transformer models address their runtime and memory\\nchallenges for long sequences.\\nMany approximate attention methods have aimed to reduce the compute and memory requirements of\\nattention. These methods range from sparse-approximation [ 51,74] to low-rank approximation [ 12,50,84],\\nand their combinations [ 3,9,92]. Although these methods reduce the compute requirements to linear or\\nnear-linear in sequence length, many of them do not display wall-clock speedup against standard attention\\nand have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not\\ncorrelate with wall-clock speed) and tend to ignore overheads from memory access (IO).\\nIn this paper, we argue that a missing principle is making attention algorithms IO-aware [1]—that is,', metadata={'source': 'new_papers/new_papers/Flash-attention.pdf', 'page': 0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhs0C0FYASlM"
   },
   "source": [
    "## HF Instructor Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Emj46ATxtV9C",
    "outputId": "b6e2ae16-910c-4a98-f24a-9fbbb78a25e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
    "                                                      model_kwargs={\"device\": \"cuda\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsYsIy8F4cdm"
   },
   "source": [
    "## create the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 229373,
     "status": "ok",
     "timestamp": 1718348985902,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Q_eTIZwf4Dk2"
   },
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "\n",
    "## Here is the new embeddings being used\n",
    "embedding = instructor_embeddings\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=texts,\n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1718348985903,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "uRfD_Te-47lb",
    "outputId": "ffc19942-2481-4eba-808e-62439d9d4471"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1718348985903,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "A-h1y_eAHmD-"
   },
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal.\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siLXR-XT0JoI"
   },
   "source": [
    "## Make a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1718348985903,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "6ObunFU30Lxh"
   },
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1718348985903,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "cYA-H59u0Skn",
    "outputId": "99c6e241-4ae4-4c14-b9f6-204a00e48f9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Flash attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1718348985903,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "h0iAuh_B0ZjE",
    "outputId": "a3001866-bfef-4c26-dbcd-100c0aa40ac8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1718348985904,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Vws1udbPaPmQ",
    "outputId": "c12060f7-f601-4489-ee4b-050f66d89980"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='access.\\nWe propose FlashAttention , a new attention algorithm that computes exact attention with far fewer\\nmemory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.\\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large\\nintermediate attention matrix for the backward pass. We apply two well-established techniques to address\\nthese challenges. (i) We restructure the attention computation to split the input into blocks and make several\\npasses over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\\nstore the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\\nbackward pass, which is faster than the standard approach of reading the intermediate attention matrix from\\nHBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and', metadata={'page': 1, 'source': 'new_papers/new_papers/Flash-attention.pdf'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1718348985904,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "jVWgPJXs1yRq"
   },
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1718348985904,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "H4N0IhRM0hHL",
    "outputId": "b02325e5-4d74-464d-b035-5adbc0ee88aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1718348985904,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "4jXL9u-u0prF",
    "outputId": "e414b373-2d04-493c-cd0b-682ec2421154"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 3}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ia-4OXa5IeP"
   },
   "source": [
    "## Make a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "error",
     "timestamp": 1718349212143,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "MGx8XblM4shW",
    "outputId": "e5f1913a-dba4-4251-b8b2-f4e22b080053"
   },
   "outputs": [],
   "source": [
    "# create the chain to answer questions\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=local_llm,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  retriever=retriever,\n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212143,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "LZEo26mw8e5k"
   },
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212144,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "wKfX4vX-5RFT"
   },
   "outputs": [],
   "source": [
    "# full example\n",
    "query = \"What is Flash attention?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212144,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "olRm73t3rNt2"
   },
   "outputs": [],
   "source": [
    "# break it down\n",
    "query = \"What does IO-aware mean?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)\n",
    "# llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212144,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "wg-e6fh6rNwz"
   },
   "outputs": [],
   "source": [
    "query = \"What is tiling in flash-attention?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212144,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "cuFf8D-rrN0I"
   },
   "outputs": [],
   "source": [
    "query = \"What is toolformer?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212144,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "yMyu13ouwgqs"
   },
   "outputs": [],
   "source": [
    "query = \"What tools can be used with toolformer?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212144,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "t5KETxphrN3d"
   },
   "outputs": [],
   "source": [
    "query = \"How many examples do we need to provide for each tool?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212144,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "692pHNkFrN5z"
   },
   "outputs": [],
   "source": [
    "query = \"What are the best retrieval augmentations for LLMs?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1718349212144,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Dl7hy8IvFFL0"
   },
   "outputs": [],
   "source": [
    "query = \"What are the differences between REALM and RAG?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212145,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "IPIhZWAR5n3X"
   },
   "outputs": [],
   "source": [
    "qa_chain.retriever.search_type , qa_chain.retriever.vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212145,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "3_lp0_796P_-"
   },
   "outputs": [],
   "source": [
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSxVCnNi5h1-"
   },
   "source": [
    "## Deleteing the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212145,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "H7xmepGJ2GAE"
   },
   "outputs": [],
   "source": [
    "!zip -r db.zip ./db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212145,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Jl84qGQt5Wu5"
   },
   "outputs": [],
   "source": [
    "# To cleanup, you can delete the collection\n",
    "vectordb.delete_collection()\n",
    "vectordb.persist()\n",
    "\n",
    "# delete the directory\n",
    "!rm -rf db/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2r0ZIBPJp-K"
   },
   "source": [
    "## Starting again loading the db\n",
    "\n",
    "restart the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212145,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "8pc7CM_mTQAt"
   },
   "outputs": [],
   "source": [
    "!unzip db.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1718349212145,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "us3F8ZKeRiz2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1718349212146,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "qK1nY4PkKYGo"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1718349212146,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "396RyNbS4EXx"
   },
   "outputs": [],
   "source": [
    "persist_directory = 'db'\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb2 = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=embedding,\n",
    "                   )\n",
    "\n",
    "retriever = vectordb2.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1718349212146,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "F3vkSxxYKCZ9"
   },
   "outputs": [],
   "source": [
    "# Set up the turbo LLM\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "aborted",
     "timestamp": 1718348517411,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "PsR60NH5KCfj"
   },
   "outputs": [],
   "source": [
    "# create the chain to answer questions\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  retriever=retriever,\n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1718348517412,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "RWulTG0eKCfk"
   },
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1718348517412,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "mDp-g2FtKCfk"
   },
   "outputs": [],
   "source": [
    "# full example\n",
    "query = \"How much money did Pando raise?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fPl26c-TbWw"
   },
   "source": [
    "### Chat prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1718348517412,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "wwyuhrpu5XqM"
   },
   "outputs": [],
   "source": [
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1718348517412,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "LcWXvSCHRvHO"
   },
   "outputs": [],
   "source": [
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1718348517412,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "978QWCeJSRdu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1GztZv8jOpvXk5KBd7heODz-oKe6fkCwA",
     "timestamp": 1718245284924
    },
    {
     "file_id": "1PeqXdCM6IU9qnTMdHyUIEZajQBSeSkXM",
     "timestamp": 1683811643108
    },
    {
     "file_id": "1_Ei_y2qPct07dKQHKNHkXl8LHLehfd8w",
     "timestamp": 1683730009255
    },
    {
     "file_id": "1BO8UHPNzQR7t_8AnOY9S3yr6Xk4XUHyb",
     "timestamp": 1683705272518
    },
    {
     "file_id": "1gBBk8ATdmsq6jV2ayd4SFxaTDTu81Qb3",
     "timestamp": 1683626576221
    },
    {
     "file_id": "1gWrd0P4f4DloYX0_ds4NiRUiXGO8rI7l",
     "timestamp": 1683592210355
    },
    {
     "file_id": "1DvVjynVZYvhJQp4yZbLXLByhqRdu6GkB",
     "timestamp": 1683544314022
    },
    {
     "file_id": "1BT_kRFMP27lmwAoWeIhiie0VPs0oqShz",
     "timestamp": 1683524040132
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
