{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THA-ZVix2roX"
   },
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "**LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It is designed to support the development of complex chains with advanced features such as streaming, async, and parallel execution.** LCEL allows for the creation of chains that integrate seamlessly with LangSmith and LangServe, making it easier to go from prototype to production. Key features of LCEL include:\n",
    "\n",
    "1. **Streaming Support**: LCEL chains can stream tokens straight from a language model to a streaming output parser, providing fast time-to-first-token.\n",
    "2. **Async Support**: Chains built with LCEL can be called both synchronously and asynchronously, making them suitable for prototypes and production environments.\n",
    "3. **Parallel Execution**: LCEL automatically executes steps in parallel when possible, reducing latency.\n",
    "4. **Retries and Fallbacks**: LCEL allows for configuring retries and fallbacks for any part of the chain, enhancing reliability.\n",
    "5. **Access to Intermediate Results**: Intermediate results can be accessed and streamed, useful for debugging and providing feedback to users.\n",
    "6. **Input and Output Schemas**: LCEL chains have Pydantic and JSONSchema schemas inferred from the chain structure, enabling input and output validation.\n",
    "7. **Seamless LangSmith Tracing**: All steps in an LCEL chain are automatically logged to LangSmith for maximum observability and debuggability.\n",
    "8. **Seamless LangServe Deployment**: Any chain created with LCEL can be easily deployed using LangServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63283,
     "status": "ok",
     "timestamp": 1719216483681,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "RRYSu48huSUW",
    "outputId": "57514f65-0380-4e08-c58d-019a0532cb4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip -q install langchain huggingface_hub openai tiktoken\n",
    "!pip -q install chromadb duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXYo9vrzRVIB"
   },
   "source": [
    "# **LangSmith**\n",
    "\n",
    "**LangSmith is a unified DevOps platform designed to help developers move their language model (LLM) applications from prototype to production. It provides a comprehensive suite of tools for developing, collaborating, testing, deploying, and monitoring LLM applications**. Key features of LangSmith include:\n",
    "\n",
    "1. **Debugging and Tracing**: LangSmith offers full visibility into the entire sequence of calls, allowing developers to spot the source of errors and performance bottlenecks in real-time. It also supports debugging and tracing of LLM calls, making it easier to identify and fix issues.\n",
    "\n",
    "2. **Collaboration Tools**: The platform includes features such as LangSmith Hub for crafting and versioning prompts, Annotation Queues for adding human labels and feedback on traces, and Datasets for collecting examples and constructing datasets from production data or existing sources.\n",
    "\n",
    "3. **Evaluation and Testing**: LangSmith integrates seamlessly with open-source evaluation modules, including heuristic and LLM-based evaluations. It also supports automated feedback on individual runs and provides tools for evaluating and auditing workflows.\n",
    "\n",
    "4. **Deployment and Monitoring**: LangSmith allows for one-click deployment of applications using LangServe, which includes features like parallelization, fallbacks, batch, streaming, and async support. It also provides monitoring tools to track cost, latency, and quality, enabling developers to take action when needed.\n",
    "\n",
    "5. **Security and Data Ownership**: LangSmith ensures data security by storing traces in a Clickhouse database encrypted in transit and at rest. Additionally, users own all rights to their data, and LangSmith does not train on user data.\n",
    "\n",
    "6. **Integration and SDKs**: LangSmith offers SDKs in Python and TypeScript, making it easy to integrate with various frameworks and applications. It also supports logging traces via the API, allowing for programmatic interaction with all features.\n",
    "\n",
    "Overall, LangSmith is designed to streamline the development and deployment of LLM applications, providing a unified platform for developers to build, test, and deploy reliable and efficient language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2dltAUpRg_k"
   },
   "source": [
    "# **LangServe**\n",
    "\n",
    "**LangServe is a Python library designed to help developers deploy LangChain runnables and chains as REST APIs. It is integrated with FastAPI and uses Pydantic for data validation.** LangServe simplifies the process of deploying LangChain applications by providing features such as:\n",
    "\n",
    "1. **Automatic API Generation**: LangServe automatically generates APIs for LangChain runnables and chains, making it easy to expose these applications over a REST interface.\n",
    "2. **Input and Output Schemas**: LangServe infers input and output schemas from the LangChain objects and enforces them on every API call, providing rich error messages.\n",
    "3. **Efficient Endpoints**: LangServe provides efficient endpoints for invoking, batching, and streaming requests, allowing for many concurrent requests on a single server.\n",
    "4. **Playground and Tracing**: LangServe includes a playground page for testing and debugging, and it supports tracing to LangSmith for monitoring and debugging production deployments.\n",
    "5. **Client SDK**: LangServe provides a client SDK that can be used to call the deployed APIs as if they were running locally.\n",
    "6. **Hosted Version**: A hosted version of LangServe is planned for one-click deployments of LangChain applications.\n",
    "\n",
    "LangServe is designed to streamline the deployment of LangChain applications, making it easier to go from prototype to production. It is particularly useful for building and deploying AI language model applications, such as chatbots and other AI-powered tools.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1719216483681,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "dNA4TsHpu6OM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your openai api key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1719216483682,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "J-KFB7J_u_3L",
    "outputId": "e397dc86-0b3c-4f99-c7ad-fb569ae5a5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.2.5\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: aiohttp, async-timeout, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqwsGJDhvAQ5"
   },
   "source": [
    "# LangChain Expression Language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1719216483682,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "-zns220a25qO",
    "outputId": "4eecf06f-8ca4-4682-d5a9-8f88765543b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.9)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.81)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (2.7.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (2.18.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.5 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 860,
     "status": "ok",
     "timestamp": 1719216484536,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "IfCt8bhHNu9u"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1719216484536,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "P_Vz09usvqhb",
    "outputId": "443fad85-4ccf-4208-8a33-67005b2d2cc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    "    )\n",
    "\n",
    "model2 = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1719216484536,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Z6HVNGkvv9-G"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me an intersting fact about {subject}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1719216484537,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Hil5bkKFwCha"
   },
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1719216484537,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "cm8y8Ll4wJMH",
    "outputId": "27bf8262-65e4-4bd7-a5ec-a8b86794f826"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Elvis Presley was known for his iconic hairstyle, which he styled himself using a combination of Vaseline and rose oil to achieve his signature slicked-back look.', response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 16, 'total_tokens': 49}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-082cf832-ad38-4247-9232-cc3cd99e8ffc-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"Elvis\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIckh1nlSpn9"
   },
   "source": [
    "The `StrOutputParser` is a component in the LangChain framework designed to convert the output of language models (LLMs) into straightforward strings. This conversion is essential for applications that require a simple, text-based interpretation of the model's response. Here are the key points about `StrOutputParser`:\n",
    "\n",
    "### Functionality\n",
    "\n",
    "- **Simple Conversion**: The `StrOutputParser` takes the output of a language model and converts it into a string. If the model already outputs a string, it passes this through unchanged. For ChatModels, which output a structured message, the parser extracts the `.content` attribute, ensuring a consistent string output.\n",
    "- **Versatility**: This parser is versatile, supporting both LLMs and ChatModels, making it a go-to choice for a wide range of applications within the LangChain ecosystem.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Extracting Textual Responses**: The `StrOutputParser` is particularly useful in applications where the raw output of a language model needs to be simplified or standardized. Examples include extracting textual responses from chat-based models for logging or display purposes.\n",
    "- **Preparing Model Outputs**: It is also useful for preparing model outputs for further text-based processing or analysis, such as sentiment analysis or keyword extraction.\n",
    "\n",
    "### Integration with LangChain\n",
    "\n",
    "- **Initialization**: Instantiate the `StrOutputParser` within your LangChain application.\n",
    "- **Integration**: Integrate the parser into your LLM chain by adding it as a step in the sequence of operations that process the LLM's output.\n",
    "- **Usage**: Invoke the chain with your desired input. The `StrOutputParser` will automatically convert the LLM's output into a string.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Stream Support**: The `StrOutputParser` supports streaming, allowing it to process streamed chunks from previous steps as they are generated.\n",
    "- **Customization**: The parser's behavior can be customized to fit the specific needs of your application, ensuring the output is precisely what you need.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Define the LLM chain\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the chain with input\n",
    "result = chain.invoke({\"input\": \"Your question here\"})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "In summary, the `StrOutputParser` is an essential tool for developers working with language models, providing a simple yet effective solution for converting complex model outputs into usable strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719216492261,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "UoeILxMtwS-A"
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1194,
     "status": "ok",
     "timestamp": 1719216495677,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "-MAwnHOTwlw1",
    "outputId": "db52564f-b239-4e2c-cbb4-4e366b299e54"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Elvis Presley was known for his iconic hairstyle, which he maintained by using a combination of hair dye and Vaseline to achieve his signature slicked-back look.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"Elvis\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1719216564963,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "riZRBZfcRrmg"
   },
   "outputs": [],
   "source": [
    "chain = prompt | model2 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 2595,
     "status": "ok",
     "timestamp": 1719216570016,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "3Hkzc57cRuYG",
    "outputId": "30f85fc1-ba4e-49c0-d2a1-f47bbcad037d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'One interesting fact about Elvis Presley is that he was a black belt in karate. Elvis began studying martial arts in 1958 while he was stationed in Germany during his military service. He trained under various instructors and eventually earned his black belt in 1960. Elvis was passionate about karate and even incorporated some of its moves into his stage performances. He continued to practice and promote martial arts throughout his life, and his interest in the discipline influenced his personal philosophy and physical fitness regimen.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"Elvis\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z8VleV0wzto"
   },
   "source": [
    "## Bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1719216574347,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "avT5iyzbxC6N"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me 3 intersting facts about {subject}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719216574830,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "j0cDP5Whwm31"
   },
   "outputs": [],
   "source": [
    "chain = prompt | model.bind(stop=[\"\\n\"]) | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1719216579619,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "ynNgP_4Fw6na",
    "outputId": "39bdb80a-1989-47f8-eb5f-47e6a48ae101"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1. Elvis Presley was known as the \"King of Rock and Roll\" and is one of the best-selling solo artists in the history of recorded music.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"Elvis\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVV-Wa8AxVuL"
   },
   "source": [
    "## Adding OpenAI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719216580288,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Grw9Z1ihxKgt"
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"joke\",\n",
    "      \"description\": \"A joke\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"setup\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The setup for the joke\"\n",
    "          },\n",
    "          \"punchline\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The punchline for the joke\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"setup\", \"punchline\"]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "functions_chain = prompt | model.bind(function_call= {\"name\": \"joke\"}, functions= functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1435,
     "status": "ok",
     "timestamp": 1719216584847,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "AL0Td6_nxJhc",
    "outputId": "7db775fb-9909-4a27-a12d-60047b0cadad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"setup\":\"Why don\\'t bears like fast food?\",\"punchline\":\"Because they can\\'t catch it!\"}', 'name': 'joke'}}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 81, 'total_tokens': 105}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bb274e64-4702-4acd-bbac-5f60c0e890e3-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_chain.invoke({\"subject\": \"bears\"}, config={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7B7bxCJyA9a"
   },
   "source": [
    "### Functions Output Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1719216588344,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "OgSLlfvMxwms"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "functions_chain = (\n",
    "    prompt\n",
    "    | model.bind(function_call= {\"name\": \"joke\"}, functions= functions)\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1719216591985,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "DeX7OkTvyqx5",
    "outputId": "0575b8c7-92f6-4a01-d093-9cbf9e312583"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't bears wear shoes?\",\n",
       " 'punchline': 'Because they have bear feet!'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = functions_chain.invoke({\"subject\": \"bears\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719216592583,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "GyqJpMyGy0LR",
    "outputId": "f30aa526-8a07-43c3-91ac-52299f81104f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Because they have bear feet!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['punchline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1719216597823,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "eLfCEKN5zFVA"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "functions_chain = (\n",
    "    prompt\n",
    "    | model.bind(function_call= {\"name\": \"joke\"}, functions= functions)\n",
    "    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 10404,
     "status": "ok",
     "timestamp": 1719216610917,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "x_jdQSAAzbOo",
    "outputId": "73c4e7c4-6661-41ca-8712-dd50b50bcf66"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Why don't bears wear shoes?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_chain.invoke({\"subject\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mag27JElztH5"
   },
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNnlBfTkVNb5"
   },
   "source": [
    "The `RunnablePassthrough` is a type of runnable in the LangChain framework that allows you to pass inputs unchanged through a chain. It behaves like the identity function, ensuring that the input is not modified as it moves through the chain. Here are the key points about `RunnablePassthrough`:\n",
    "\n",
    "### Functionality\n",
    "\n",
    "- **Unchanged Input**: `RunnablePassthrough` ensures that the input remains unchanged as it passes through the chain, making it useful for scenarios where the input needs to be preserved.\n",
    "\n",
    "- **Adding Keys**: While the input remains unchanged, `RunnablePassthrough` can be configured to add additional keys to the output if the input is a dictionary. This is achieved using the `assign` method.\n",
    "\n",
    "### Usage\n",
    "\n",
    "- **Chaining**: `RunnablePassthrough` is typically used in conjunction with other runnables, such as `RunnableParallel`, to create more complex chains.\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "  from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "  runnable = RunnableParallel(\n",
    "      passed=RunnablePassthrough(),\n",
    "      modified=lambda x: x[\"num\"] + 1\n",
    "  )\n",
    "\n",
    "  result = runnable.invoke({\"num\": 1})\n",
    "  print(result)  # Output: {'passed': {'num': 1}, 'modified': 2}\n",
    "  ```\n",
    "\n",
    "### Importance\n",
    "\n",
    "- **Preserving Input**: `RunnablePassthrough` is essential for preserving the original input in a chain, ensuring that it is not modified unintentionally.\n",
    "\n",
    "- **Flexibility**: It provides flexibility in creating chains by allowing the addition of new keys to the output while keeping the input intact.\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "- **TypeError**: A common issue encountered when using `RunnablePassthrough` is a `TypeError` due to incorrect chaining or incompatible types. This can be resolved by ensuring that the input is a dictionary and that the chaining is done correctly.\n",
    "\n",
    "### Integration with LangChain\n",
    "\n",
    "- **LangChain Components**: `RunnablePassthrough` is part of the LangChain framework and can be used in conjunction with other components such as `RunnableParallel`, `RunnableLambda`, and `StrOutputParser`.\n",
    "\n",
    "- **Chaining**: It is designed to be used in chains, allowing for the creation of complex workflows that involve multiple runnables.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define a chain with RunnablePassthrough\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"question\": \"Where did Harrison work?\"})\n",
    "print(result)  # Output: 'Harrison worked at Kensho.'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1gCBQFNVQfD"
   },
   "source": [
    "The `itemgetter` function in Python is used to extract specific items from a list or dictionary. It is often used in conjunction with LangChain to extract specific keys from a dictionary or map. Here are some key points about `itemgetter`:\n",
    "\n",
    "### Usage\n",
    "\n",
    "- **Extracting Keys**: `itemgetter` is used to extract specific keys from a dictionary or map. For example, `itemgetter(\"key\")` would extract the value associated with the key \"key\" from a dictionary.\n",
    "\n",
    "- **Combining with Runnables**: `itemgetter` can be combined with LangChain runnables to extract specific keys from the output of a previous step. For example, `{\"context\": itemgetter(\"question\") | retriever}` would extract the value associated with the key \"question\" from the input and pass it to the `retriever` runnable.\n",
    "\n",
    "- **Parallel Execution**: `itemgetter` can be used with `RunnableParallel` to extract specific keys from multiple runnables executed in parallel. For example, `RunnableParallel({\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")})` would extract the values associated with the keys \"question\" and \"context\" from the input and pass them to the `retriever` runnable in parallel.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "retriever = ...  # Define the retriever runnable\n",
    "\n",
    "chain = RunnableParallel({\n",
    "    \"context\": itemgetter(\"question\") | retriever,\n",
    "    \"question\": itemgetter(\"question\")\n",
    "})\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(result)  # Output: {'context': ..., 'question': ...}\n",
    "```\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "- **TypeError**: A common issue encountered when using `itemgetter` is a `TypeError` due to incorrect usage or incompatible types. This can be resolved by ensuring that `itemgetter` is used correctly and that the types are compatible.\n",
    "\n",
    "### Integration with LangChain\n",
    "\n",
    "- **LangChain Components**: `itemgetter` is part of the Python standard library and can be used in conjunction with LangChain components such as `RunnableParallel`, `RunnablePassthrough`, and `StrOutputParser`.\n",
    "\n",
    "- **Chaining**: `itemgetter` is designed to be used in chains, allowing for the creation of complex workflows that involve multiple runnables.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = RunnableParallel({\n",
    "    \"context\": itemgetter(\"question\") | retriever,\n",
    "    \"question\": itemgetter(\"question\")\n",
    "}) | prompt | model | StrOutputParser()\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(result)  # Output: {'context': ..., 'question': ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1719217282969,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "gw24xjUkzf7q"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 710,
     "status": "ok",
     "timestamp": 1719217285839,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "ZarTWbugMprG"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1719217291203,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "xCTfodQoNAfy"
   },
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "fake_docs = [\"James bond works for MI6\",\"Bond is a spy\",\n",
    "             \"James Bond has a licence to kill\", \"James Bond likes cats\"]\n",
    "vectorstore = Chroma.from_texts(fake_docs, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1719217297212,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "zYE7T7npNEwu"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1719217301429,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "UER4z9TmNE4r"
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1719217313668,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "Lgdva60wOylW",
    "outputId": "53040b3d-380c-46ef-86c6-0845da015206"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'James Bond is a spy who works for MI6.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who is James Bond?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1719217314463,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "h8rCZiKwbnYD",
    "outputId": "9f8320d2-16fa-4901-8269-eeb001311b9a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'James Bond likes cats.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What does James Bond like to do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1719217319449,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "COxxdA0POyoX"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\n",
    "    \"context\": itemgetter(\"question\") | retriever,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"language\": itemgetter(\"language\")\n",
    "} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1323,
     "status": "ok",
     "timestamp": 1719217324839,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "XH-elG-BOyrp",
    "outputId": "a41442ea-636f-48a1-9edb-403cf86bd89d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'James Bond works for MI6.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"where does James work?\", \"language\": \"english\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 2077,
     "status": "ok",
     "timestamp": 1719217327485,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "dm-9KovTcO5g",
    "outputId": "0885a991-c33e-4df4-feb7-5ac6d00be086"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'James Bond lavora per MI6.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"where does James work?\", \"language\": \"italian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v06mU7PBSMfd"
   },
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1719217329216,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "8TumPUpLSN2n"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1719217330952,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "EXC03bY0SOlJ"
   },
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1719217331905,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "SVBb4qnRSRPC"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"turn the following user input into a search query for a search engine:\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1719217333720,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "w1w4U9TNSd6C"
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1719217335734,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "d4LYnKffSeWK",
    "outputId": "edaa95f8-dd6d-45c1-dc0d-c7b98b274683"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'No good DuckDuckGo Search Result was found'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"Who played james bond first\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1719217342576,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "dOi5DbEJUopT",
    "outputId": "a4f0a260-871d-4472-834d-059056ddef84"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\"actor who played James Bond last\"'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | StrOutputParser()\n",
    "chain.invoke({\"input\": \"Who played james bond last\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5Z6tSxO0Z-H"
   },
   "source": [
    "## Arbitary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohKfgz70XUwL"
   },
   "source": [
    "The `RunnableLambda` is a fundamental component in the LangChain framework that allows you to convert Python functions into LangChain-compatible runnables. Here are the key points about `RunnableLambda`:\n",
    "\n",
    "### Functionality\n",
    "\n",
    "- **Converting Functions**: `RunnableLambda` takes a Python function as input and converts it into a LangChain-compatible runnable. This allows you to integrate custom functions into LangChain pipelines.\n",
    "\n",
    "- **Sync and Async Support**: `RunnableLambda` supports both synchronous and asynchronous execution. It can be used with synchronous functions by default and can also be configured to use asynchronous functions.\n",
    "\n",
    "- **Streaming**: `RunnableLambda` does not support streaming by default. If you need to support streaming, you should use `RunnableGenerator` instead.\n",
    "\n",
    "- **RunnableConfig**: `RunnableLambda` can optionally accept a `RunnableConfig` object, which allows you to pass callbacks, tags, and other configuration information to nested runs.\n",
    "\n",
    "### Usage\n",
    "\n",
    "- **Creating a RunnableLambda**:\n",
    "  ```python\n",
    "  from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "  def add_one(x: int) -> int:\n",
    "      return x + 1\n",
    "\n",
    "  runnable = RunnableLambda(add_one)\n",
    "  ```\n",
    "\n",
    "- **Chaining**:\n",
    "  ```python\n",
    "  from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "  def add_one(x: int) -> int:\n",
    "      return x + 1\n",
    "\n",
    "  def multiply_by_two(x: int) -> int:\n",
    "      return x * 2\n",
    "\n",
    "  add_one_runnable = RunnableLambda(add_one)\n",
    "  multiply_by_two_runnable = RunnableLambda(multiply_by_two)\n",
    "\n",
    "  chain = add_one_runnable | multiply_by_two_runnable\n",
    "  ```\n",
    "\n",
    "- **Invoking the Chain**:\n",
    "  ```python\n",
    "  result = chain.invoke(1)  # Returns 4\n",
    "  ```\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def extract_fact(x: str) -> str:\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "get_fact_runnable = RunnableLambda(extract_fact)\n",
    "\n",
    "chain = prompt | model | output_parser | get_fact_runnable\n",
    "\n",
    "result = chain.invoke({\"input\": \"Tell me a short fact about AI.\"})\n",
    "print(result)  # Output: The fact itself\n",
    "```\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "- **TypeError**: A common issue encountered when using `RunnableLambda` is a `TypeError` due to incorrect usage or incompatible types. This can be resolved by ensuring that the function passed to `RunnableLambda` is correctly defined and that the types are compatible.\n",
    "\n",
    "### Integration with LangChain\n",
    "\n",
    "- **LangChain Components**: `RunnableLambda` is part of the LangChain framework and can be used in conjunction with other LangChain components such as `RunnableParallel`, `RunnablePassthrough`, and `StrOutputParser`.\n",
    "\n",
    "- **Chaining**: `RunnableLambda` is designed to be used in chains, allowing for the creation of complex workflows that involve multiple runnables.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def length_function(text: str) -> int:\n",
    "    return len(text)\n",
    "\n",
    "def multiple_length_function(_dict: dict) -> int:\n",
    "    return len(_dict[\"text1\"]) * len(_dict[\"text2\"])\n",
    "\n",
    "length_runnable = RunnableLambda(length_function)\n",
    "multiple_length_runnable = RunnableLambda(multiple_length_function)\n",
    "\n",
    "chain = (\n",
    "    {\"a\": itemgetter(\"foo\") | length_runnable},\n",
    "    {\"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")} | multiple_length_runnable},\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})\n",
    "print(result)  # Output: {'a': 3, 'b': 9}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1719217350290,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "yxMzLOXaeh-s"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = {\n",
    "    \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "    \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")} | RunnableLambda(multiple_length_function)\n",
    "} | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1719217355891,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "H1P-wqvwepFE",
    "outputId": "7bb0c7e1-e891-4d19-df2a-2c2b380b7728"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='4 + 16 equals 20.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a61a390a-33f9-472b-a2aa-85134b13f679-0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"foo\": \"bars\", \"bar\": \"gahs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1719092505002,
     "user": {
      "displayName": "Harsh Tomar",
      "userId": "07306423374395310283"
     },
     "user_tz": -330
    },
    "id": "amUsxUg4Tw17"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1yUz-2IChNRnjZAQBJmdxGSFHv29Vc3Dr",
     "timestamp": 1719091733909
    },
    {
     "file_id": "1KlS3cxYQ408EpFAwlpyjkwava8fFKrIn",
     "timestamp": 1691065688370
    },
    {
     "file_id": "1BT_kRFMP27lmwAoWeIhiie0VPs0oqShz",
     "timestamp": 1690977179453
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
