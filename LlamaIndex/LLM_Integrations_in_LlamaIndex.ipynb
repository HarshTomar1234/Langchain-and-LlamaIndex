{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyuP4wj36FsH"
      },
      "source": [
        "# LlamaIndex LLM integrations\n",
        "\n",
        "LlamaIndex exposes a common interface for all their LLM integrations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **1. What are LLM Integrations in LlamaIndex?**\n",
        "LlamaIndex acts as a bridge between your data and Large Language Models (LLMs). It allows you to:\n",
        "- **Index** unstructured/structured data.\n",
        "- **Query** the indexed data using LLMs.\n",
        "- **Augment** LLMs with external knowledge.\n",
        "\n",
        "**LLM Integrations** refer to how LlamaIndex interacts with different LLMs (OpenAI, Hugging Face, Anthropic, etc.) to generate responses, perform retrieval-augmented generation (RAG), or fine-tune models.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Types of LLM Integrations**\n",
        "We’ll cover:\n",
        "- OpenAI\n",
        "- Hugging Face (Local Models)\n",
        "- Anthropic\n",
        "- Custom LLMs\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1 OpenAI Integration**\n",
        "**Theory**:\n",
        "- Use OpenAI’s GPT models (e.g., `gpt-3.5-turbo`, `gpt-4`) via API.\n",
        "- Handles **text completion**, **chat**, and **embeddings**.\n",
        "\n",
        "#### **Code Example**:\n",
        "```python\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "# Set API Key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# Load data and create index\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Query using the LLM\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "response = query_engine.query(\"What is the capital of France?\")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- `temperature=0.7`: Controls randomness (0 = deterministic, 1 = creative).\n",
        "- The LLM generates responses based on the indexed data.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.2 Hugging Face Integration (Local Models)**\n",
        "**Theory**:\n",
        "- Run open-source models locally (e.g., `Llama-2`, `BERT`).\n",
        "- Useful for privacy, cost savings, or custom fine-tuning.\n",
        "\n",
        "#### **Code Example**:\n",
        "```python\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load a Hugging Face model\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Initialize LLM\n",
        "llm = HuggingFaceLLM(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    device_map=\"auto\",  # Use GPU if available\n",
        ")\n",
        "\n",
        "# Use in a pipeline\n",
        "from llama_index import ServiceContext\n",
        "service_context = ServiceContext.from_defaults(llm=llm)\n",
        "\n",
        "# Build an index and query\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- `device_map=\"auto\"`: Automatically uses GPU (CUDA) if available.\n",
        "- Requires `transformers` and `torch` libraries.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.3 Anthropic (Claude) Integration**\n",
        "**Theory**:\n",
        "- Use Anthropic’s Claude models for safer, structured outputs.\n",
        "- Requires API access.\n",
        "\n",
        "#### **Code Example**:\n",
        "```python\n",
        "from llama_index.llms import Anthropic\n",
        "\n",
        "# Set API Key\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
        "\n",
        "# Initialize LLM\n",
        "llm = Anthropic(model=\"claude-2\", max_tokens=1000)\n",
        "\n",
        "# Query example\n",
        "response = llm.complete(\"Explain quantum mechanics in simple terms.\")\n",
        "print(response.text)\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- `max_tokens`: Limits response length.\n",
        "- Claude is optimized for helpfulness and harmlessness.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.4 Custom LLM Integration**\n",
        "**Theory**:\n",
        "- Wrap any LLM (e.g., a proprietary model) into LlamaIndex.\n",
        "- Subclass `CustomLLM` and implement required methods.\n",
        "\n",
        "#### **Code Example**:\n",
        "```python\n",
        "from llama_index.llms import CustomLLM, CompletionResponse\n",
        "\n",
        "class MyCustomLLM(CustomLLM):\n",
        "    def __init__(self):\n",
        "        self.model = my_custom_model  # Replace with your model\n",
        "\n",
        "    def complete(self, prompt: str) -> CompletionResponse:\n",
        "        response = self.model.generate(prompt)\n",
        "        return CompletionResponse(text=response)\n",
        "\n",
        "# Initialize and use\n",
        "custom_llm = MyCustomLLM()\n",
        "service_context = ServiceContext.from_defaults(llm=custom_llm)\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- Replace `my_custom_model` with your model’s inference logic.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Key Concepts**\n",
        "### **3.1 ServiceContext**\n",
        "- Central configuration for LLMs, embeddings, and more.\n",
        "```python\n",
        "from llama_index import ServiceContext\n",
        "service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\")\n",
        "```\n",
        "\n",
        "### **3.2 Temperature & Token Limits**\n",
        "- **Temperature**: Higher values (e.g., 0.8) increase creativity.\n",
        "- **Max Tokens**: Controls response length.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Choosing the Right LLM**\n",
        "| **LLM**          | **Use Case**                          | **Pros**                          | **Cons**                |\n",
        "|-------------------|---------------------------------------|-----------------------------------|-------------------------|\n",
        "| OpenAI GPT        | General-purpose, high-quality outputs | Easy integration, powerful        | Cost, API dependency    |\n",
        "| Hugging Face      | Privacy, customization                | Free, offline use                 | Resource-intensive      |\n",
        "| Anthropic Claude  | Safety-critical applications          | Structured outputs                | Limited access          |\n",
        "| Custom LLM        | Proprietary models                    | Full control                      | Development effort      |\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Advanced Integrations**\n",
        "### **5.1 Multi-Modal LLMs**\n",
        "- Combine text and images (e.g., GPT-4V).\n",
        "```python\n",
        "from llama_index.multi_modal_llms import OpenAIMultiModal\n",
        "mm_llm = OpenAIMultiModal(model=\"gpt-4-vision-preview\")\n",
        "response = mm_llm.complete(prompt=\"Describe this image:\", image=\"image.png\")\n",
        "```\n",
        "\n",
        "### **5.2 Async & Streaming**\n",
        "- Non-blocking calls and real-time responses.\n",
        "\n",
        "```python\n",
        "# Async query\n",
        "response = await query_engine.aquery(\"What is AI?\")\n",
        "# Streaming\n",
        "for chunk in response.response_gen:\n",
        "    print(chunk, end=\"\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Resources**\n",
        "- **LlamaIndex Docs**: https://docs.llamaindex.ai\n",
        "- **Hugging Face Models**: https://huggingface.co/models\n",
        "- **OpenAI API Docs**: https://platform.openai.com/docs\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9l37jrcamSXC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VRUWkfli6FsI"
      },
      "outputs": [],
      "source": [
        "%pip install -Uq 'llama-index'\n",
        "%pip install -Uq llama-index-llms-openai\n",
        "%pip install -Uq llama-index-llms-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e6Xchwh_6FsJ"
      },
      "outputs": [],
      "source": [
        "%pip install -Uq python-dotenv\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if os.path.exists('../.env'):\n",
        "    load_dotenv('../.env')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_KU-6qCr6FsJ",
        "outputId": "00af067a-c56e-47da-9dd3-899f5604a1ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama-index==0.12.15\n"
          ]
        }
      ],
      "source": [
        "!pip freeze | grep \"llama-index==\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p1W1khUZ6FsJ"
      },
      "outputs": [],
      "source": [
        "# we will use Groq models to make this free to use. but we still need an API key\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bEsaTbb-6FsK"
      },
      "outputs": [],
      "source": [
        "# we will be using pprint for logging\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lupHDW8B6FsK"
      },
      "source": [
        "## One common interface for all LLMs\n",
        "\n",
        "You can import several LLMs on LlamaIndex. LlamaIndex exposes a common interface to all LLM integrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Dc2n6y-t6FsK"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llm = Groq(model=\"gemma2-9b-it\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiqtAYxk6FsK"
      },
      "source": [
        "### The `complete()` method\n",
        "\n",
        "The `complete()` method is the main method of the interface. It is a text-to-text method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tTucr4z_6FsK",
        "outputId": "95fd8c04-f225-4752-d8e7-c897603faa9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why don't scientists trust atoms? \n",
            "\n",
            "Because they make up everything! 😄  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = llm.complete(\"Tell me a joke\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O1IX3_Mx6FsL",
        "outputId": "30362eec-1b7c-4bbd-d3a6-102166c6f093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "llama_index.core.base.llms.types.CompletionResponse"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>llama_index.core.base.llms.types.CompletionResponse</b><br/>def __init__(self, /, **data: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/llama_index/core/base/llms/types.py</a>Completion response.\n",
              "\n",
              "Fields:\n",
              "    text: Text content of the response if not streaming, or if streaming,\n",
              "        the current extent of streamed text.\n",
              "    additional_kwargs: Additional information on the response(i.e. token\n",
              "        counts, function calling information).\n",
              "    raw: Optional raw JSON that was parsed to populate text, if relevant.\n",
              "    delta: New text that just streamed in (only relevant when streaming).</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 253);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OwjaGH-D6FsL",
        "outputId": "9585dbea-49d0-40de-d17d-15f797d92496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'additional_kwargs': {},\n",
            " 'delta': None,\n",
            " 'logprobs': None,\n",
            " 'raw': ChatCompletion(id='chatcmpl-a5aadd46-5b40-4269-93a5-804fdc81a47e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything! 😄  \\n\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738699444, model='gemma2-9b-it', object='chat.completion', service_tier=None, system_fingerprint='fp_10c08bf97d', usage=CompletionUsage(completion_tokens=21, prompt_tokens=13, total_tokens=34, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.051259129, prompt_time=0.001166607, completion_time=0.038181818, total_time=0.039348425), x_groq={'id': 'req_01jk99041sfth8xnsg8p1xypwk'}),\n",
            " 'text': \"Why don't scientists trust atoms? \\n\"\n",
            "         '\\n'\n",
            "         'Because they make up everything! 😄  \\n'}\n"
          ]
        }
      ],
      "source": [
        "pprint(response.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ye5t196FsL"
      },
      "source": [
        "### The `chat()` method\n",
        "\n",
        "The `chat()` method is a message-based method. It takes in a list of messages as input and returns a message as output. This is the most common interface for chatbots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cfNFYrrs6FsL",
        "outputId": "d682fbff-4502-4324-e4d8-0bb181cc62ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: In the film \"Who Framed Roger Rabbit,\" the character who framed Roger Rabbit is Judge Doom. He is the main antagonist of the movie and orchestrates the plot to frame Roger for the murder of Marvin Acme. Judge Doom's ultimate plan is to destroy Toontown to make way for a freeway, and framing Roger is part of his scheme to achieve this goal.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"system\", content=\"You are Sherlock Holmes\"),\n",
        "    ChatMessage(role=\"user\", content=\"Who framed roger rabbit?\"),\n",
        "]\n",
        "response = llm.chat(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "As39VvQN6FsL",
        "outputId": "079f0035-a6f8-492c-b1b2-4558a013e4db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "llama_index.core.base.llms.types.ChatResponse"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>llama_index.core.base.llms.types.ChatResponse</b><br/>def __init__(self, /, **data: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/llama_index/core/base/llms/types.py</a>Chat response.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 235);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "poVgsqX66FsL",
        "outputId": "8cc47aab-38a9-4392-989e-e99551dbf09d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'additional_kwargs': {'completion_tokens': 36,\n",
            "                       'prompt_tokens': 46,\n",
            "                       'total_tokens': 82},\n",
            " 'delta': None,\n",
            " 'logprobs': None,\n",
            " 'message': ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything! \\n\\nElementary, my dear Watson.  *adjusts deerstalker*  \\n\\n\")]),\n",
            " 'raw': ChatCompletion(id='chatcmpl-f7cb7e80-aa4b-4f10-b6a1-b1801f3d7f43', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything! \\n\\nElementary, my dear Watson.  *adjusts deerstalker*  \\n\\n\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738699444, model='gemma2-9b-it', object='chat.completion', service_tier=None, system_fingerprint='fp_10c08bf97d', usage=CompletionUsage(completion_tokens=36, prompt_tokens=46, total_tokens=82, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.021564835999999997, prompt_time=0.002737642, completion_time=0.065454545, total_time=0.068192187), x_groq={'id': 'req_01jk99048zf6x9k9bkf3x0gz4z'})}\n"
          ]
        }
      ],
      "source": [
        "pprint(response.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0gKjff46FsL"
      },
      "source": [
        "### Both methods have a streaming version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GVfnv6zJ6FsL"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o\")\n",
        "\n",
        "response = llm.stream_complete(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NElBVgBf6FsL",
        "outputId": "1686f025-4659-4cdf-ff23-ba59eca9ae40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WhyWhy don'tWhy don't skeletonWhy don't skeletonsWhy don't skeletons fightWhy don't skeletons fight eachWhy don't skeletons fight each otherWhy don't skeletons fight each other?\n",
            "\n",
            "Why don't skeletons fight each other?\n",
            "\n",
            "TheyWhy don't skeletons fight each other?\n",
            "\n",
            "They don'tWhy don't skeletons fight each other?\n",
            "\n",
            "They don't haveWhy don't skeletons fight each other?\n",
            "\n",
            "They don't have theWhy don't skeletons fight each other?\n",
            "\n",
            "They don't have the gutsWhy don't skeletons fight each other?\n",
            "\n",
            "They don't have the guts!Why don't skeletons fight each other?\n",
            "\n",
            "They don't have the guts!"
          ]
        }
      ],
      "source": [
        "for token in response:\n",
        "  print(token.text, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fkO5B9gD6FsL",
        "outputId": "4a4a97d4-1fd9-4992-ebea-9d54702cbe3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generator"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1Pvb6pnN6FsM"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"system\", content=\"You are Sherlock Holmes\"),\n",
        "    ChatMessage(role=\"user\", content=\"Who framed roger rabbit?\"),\n",
        "]\n",
        "\n",
        "response = llm.stream_chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cScWSlps6FsM",
        "outputId": "ff8f21a8-51d4-4bd4-e270-8c32a2e1a791",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generator"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zam_YI-D6FsM"
      },
      "source": [
        "### Structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1mIis0B26FsM"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Song(BaseModel):\n",
        "    \"\"\"Data model for a song.\"\"\"\n",
        "\n",
        "    title: str\n",
        "    length_seconds: int\n",
        "\n",
        "\n",
        "class Album(BaseModel):\n",
        "    \"\"\"Data model for an album.\"\"\"\n",
        "\n",
        "    name: str\n",
        "    artist: str\n",
        "    songs: List[Song]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rDy8kPva6FsM"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "sllm = llm.as_structured_llm(output_cls=Album)\n",
        "input_msg = ChatMessage.from_str(\"Generate an example album from The Shining\")\n",
        "\n",
        "response = sllm.chat([input_msg])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "paeYyleN6FsM",
        "outputId": "8edd4451-d038-4402-cc19-2dcee9274a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='{\"name\":\"The Shining Soundtrack\",\"artist\":\"Various Artists\",\"songs\":[{\"title\":\"Main Title\",\"length_seconds\":180},{\"title\":\"Rocky Mountains\",\"length_seconds\":210},{\"title\":\"Lonesome Ghosts\",\"length_seconds\":240},{\"title\":\"Midnight, the Stars and You\",\"length_seconds\":180},{\"title\":\"The Overlook Waltz\",\"length_seconds\":200}]}')]), raw=Album(name='The Shining Soundtrack', artist='Various Artists', songs=[Song(title='Main Title', length_seconds=180), Song(title='Rocky Mountains', length_seconds=210), Song(title='Lonesome Ghosts', length_seconds=240), Song(title='Midnight, the Stars and You', length_seconds=180), Song(title='The Overlook Waltz', length_seconds=200)]), delta=None, logprobs=None, additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OiIXeLEF6FsM",
        "outputId": "8e580e4f-c37a-4000-8d85-1b9853d7897a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'artist': 'Various Artists',\n",
            " 'name': 'The Shining Soundtrack',\n",
            " 'songs': [Song(title='Main Title', length_seconds=180),\n",
            "           Song(title='Rocky Mountains', length_seconds=210),\n",
            "           Song(title='Lonesome Ghosts', length_seconds=240),\n",
            "           Song(title='Midnight, the Stars and You', length_seconds=180),\n",
            "           Song(title='The Overlook Waltz', length_seconds=200)]}\n"
          ]
        }
      ],
      "source": [
        "pprint(response.raw.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **1. The `complete()` Method**\n",
        "### **What It Does**:\n",
        "- **`complete()`** is like asking a **single question** to the LLM and getting a **single answer**.\n",
        "- It takes a **text prompt** as input and returns a **text completion**.\n",
        "- Use this when you want a straightforward response without conversation history.\n",
        "\n",
        "### **Parameters**:\n",
        "- `prompt`: The text you want the LLM to complete.\n",
        "- `temperature`: Controls randomness (0 = factual, 1 = creative).\n",
        "- `max_tokens`: Limits the length of the response.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example 1: Basic `complete()` with OpenAI**\n",
        "```python\n",
        "from llama_index.llms import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Use complete()\n",
        "response = llm.complete(\"Explain gravity in simple terms.\")\n",
        "print(response.text)\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "```\n",
        "Gravity is the force that pulls objects toward each other. It's why things fall to the ground and why planets orbit the sun. The more mass an object has, the stronger its gravity.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Example 2: `complete()` with Hugging Face (Local LLM)**\n",
        "```python\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load a local model (e.g., Mistral-7B)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = HuggingFaceLLM(model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Use complete()\n",
        "response = llm.complete(\"What is the capital of France?\")\n",
        "print(response.text)\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "```\n",
        "The capital of France is Paris.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **2. The `chat()` Method**\n",
        "### **What It Does**:\n",
        "- **`chat()`** is like having a **conversation** with the LLM.\n",
        "- It uses a **list of messages** (with roles like `user` and `assistant`) to maintain context.\n",
        "- Ideal for multi-turn dialogues.\n",
        "\n",
        "### **Parameters**:\n",
        "- `messages`: A list of `ChatMessage` objects (e.g., `user` asks, `assistant` replies).\n",
        "- `temperature`, `max_tokens`: Same as `complete()`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example 1: Basic `chat()` with OpenAI**\n",
        "```python\n",
        "from llama_index.llms import OpenAI, ChatMessage\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define the conversation history\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hi! What’s the weather like today?\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"I don’t have real-time data. Where are you located?\"),\n",
        "    ChatMessage(role=\"user\", content=\"I’m in Paris.\")\n",
        "]\n",
        "\n",
        "# Continue the chat\n",
        "response = llm.chat(messages=messages)\n",
        "print(response.message.content)\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "```\n",
        "In Paris, the weather can vary, but you can check a weather website or app for real-time updates!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Example 2: `chat()` with Anthropic (Claude)**\n",
        "```python\n",
        "from llama_index.llms import Anthropic, ChatMessage\n",
        "\n",
        "llm = Anthropic(model=\"claude-2\")\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Write a poem about the ocean.\"),\n",
        "]\n",
        "\n",
        "response = llm.chat(messages=messages)\n",
        "print(response.message.content)\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "```\n",
        "The ocean whispers secrets deep and old,\n",
        "Its waves a dance of blue and gold...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Other Key Methods**\n",
        "\n",
        "### **3.1 `stream()`: Real-Time Responses**\n",
        "Generates responses **chunk by chunk** (useful for real-time UIs).\n",
        "\n",
        "```python\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "# Stream the response\n",
        "response = llm.stream(\"Tell me a story about a dragon.\")\n",
        "for chunk in response:\n",
        "    print(chunk.delta, end=\"\")  # Print each chunk as it arrives\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3.2 `aretrieve()` and `asynthesize()`: Async Methods**\n",
        "For non-blocking operations (useful in web apps).\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "async def async_query():\n",
        "    response = await llm.acomplete(\"Explain async programming.\")\n",
        "    print(response.text)\n",
        "\n",
        "asyncio.run(async_query())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3.3 `retrieve()`: Fetch Relevant Data**\n",
        "Retrieves data chunks from your index (used in RAG pipelines).\n",
        "\n",
        "```python\n",
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "retriever = index.as_retriever()\n",
        "\n",
        "# Retrieve relevant data for a query\n",
        "nodes = retriever.retrieve(\"What is AI?\")\n",
        "for node in nodes:\n",
        "    print(node.text)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3.4 `synthesize()`: Combine Retrieved Data + LLM**\n",
        "Generates a final answer using retrieved data and the LLM.\n",
        "\n",
        "```python\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.synthesize(\n",
        "    query=\"What is machine learning?\",\n",
        "    nodes=nodes  # Retrieved data from `retrieve()`\n",
        ")\n",
        "print(response.response)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Key Parameters Explained**\n",
        "\n",
        "| **Parameter**   | **Description**                                                                 |\n",
        "|------------------|---------------------------------------------------------------------------------|\n",
        "| `temperature`    | Controls randomness. Use `0` for facts (e.g., Q&A), `0.8` for creative writing. |\n",
        "| `max_tokens`     | Maximum length of the response (e.g., `100` for short answers).                 |\n",
        "| `top_p`          | Controls diversity (e.g., `0.9` focuses on top 90% probable tokens).            |\n",
        "| `frequency_penalty` | Reduces repetition of phrases (e.g., `0.5` discourages repetition).           |\n",
        "\n",
        "---\n",
        "\n",
        "## **5. When to Use Which Method?**\n",
        "\n",
        "| **Method**       | **Use Case**                                                                 |\n",
        "|------------------|-----------------------------------------------------------------------------|\n",
        "| `complete()`     | Single-turn Q&A, summarization, code generation.                            |\n",
        "| `chat()`         | Multi-turn conversations (e.g., chatbots, interactive dialogues).           |\n",
        "| `stream()`       | Real-time applications (e.g., ChatGPT-style typing animations).             |\n",
        "| `retrieve()`     | Fetching data from your index (without generating a response).              |\n",
        "| `synthesize()`   | Combining retrieved data with LLM to generate a final answer.               |\n",
        "| Async Methods    | Building web APIs or apps where non-blocking operations are critical.       |\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Troubleshooting Common Issues**\n",
        "1. **`max_tokens` Too Low**: Increase it if responses are cut off.\n",
        "   ```python\n",
        "   response = llm.complete(\"Explain AI...\", max_tokens=500)\n",
        "   ```\n",
        "2. **Repetitive Outputs**: Adjust `temperature` or `frequency_penalty`.\n",
        "   ```python\n",
        "   response = llm.complete(\"Write a story...\", temperature=0.8, frequency_penalty=0.5)\n",
        "   ```\n",
        "3. **API Errors**: Check your API key and internet connection.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Full Workflow Example**\n",
        "Let’s build a **RAG pipeline** using all methods:\n",
        "\n",
        "```python\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "# Step 1: Load data\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# Step 2: Index data\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Step 3: Retrieve relevant data\n",
        "retriever = index.as_retriever()\n",
        "nodes = retriever.retrieve(\"What is quantum computing?\")\n",
        "\n",
        "# Step 4: Synthesize a response\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.synthesize(query=\"What is quantum computing?\", nodes=nodes)\n",
        "print(response.response)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fX2hleMDpUvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## **1. Core LLM Methods**\n",
        "These methods interact directly with the LLM (e.g., GPT-4, Claude).\n",
        "\n",
        "### **(a) `complete()`**\n",
        "- **Purpose**: Generate a **single response** from a **text prompt**.\n",
        "- **Analogy**: Asking a chef to cook a dish (one-time request).\n",
        "- **Example**:\n",
        "  ```python\n",
        "  from llama_index.llms import OpenAI\n",
        "  \n",
        "  llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "  response = llm.complete(\"Explain the solar system.\")\n",
        "  print(response.text)\n",
        "  ```\n",
        "  **Parameters**:\n",
        "  - `prompt`: Input text.\n",
        "  - `temperature`: Creativity (0 = strict, 1 = random).\n",
        "  - `max_tokens`: Max response length.\n",
        "\n",
        "---\n",
        "\n",
        "### **(b) `chat()`**\n",
        "- **Purpose**: Have a **multi-turn conversation** using message history.\n",
        "- **Analogy**: Chatting with a friend who remembers previous messages.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  from llama_index.llms import ChatMessage\n",
        "  \n",
        "  messages = [\n",
        "      ChatMessage(role=\"user\", content=\"What’s the capital of France?\"),\n",
        "      ChatMessage(role=\"assistant\", content=\"Paris. What else can I help with?\"),\n",
        "      ChatMessage(role=\"user\", content=\"What’s its population?\")\n",
        "  ]\n",
        "  response = llm.chat(messages)\n",
        "  print(response.message.content)  # Output: \"Around 2.2 million.\"\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **(c) `stream()`**\n",
        "- **Purpose**: Stream responses **in real-time** (token by token).\n",
        "- **Analogy**: Watching a movie frame-by-frame instead of all at once.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  response = llm.stream(\"Tell me a joke.\")\n",
        "  for chunk in response:\n",
        "      print(chunk.delta, end=\"\")  # Prints tokens as they arrive\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **(d) Async Methods (`acomplete()`, `achat()`)**\n",
        "- **Purpose**: Non-blocking versions of `complete()` and `chat()`.\n",
        "- **Analogy**: Sending an email while doing other tasks.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  import asyncio\n",
        "  \n",
        "  async def async_query():\n",
        "      response = await llm.acomplete(\"What is async programming?\")\n",
        "      print(response.text)\n",
        "  \n",
        "  asyncio.run(async_query())\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Query Engine Methods**\n",
        "These methods query **indexed data** using the LLM.\n",
        "\n",
        "### **(a) `query()`**\n",
        "- **Purpose**: Ask questions about your indexed data.\n",
        "- **Analogy**: Asking a librarian to find a book and summarize it.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "  \n",
        "  # Load data and create index\n",
        "  documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "  index = VectorStoreIndex.from_documents(documents)\n",
        "  \n",
        "  # Query\n",
        "  query_engine = index.as_query_engine()\n",
        "  response = query_engine.query(\"Summarize the document.\")\n",
        "  print(response.response)\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **(b) `aquery()`**\n",
        "- **Purpose**: Async version of `query()`.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  async def async_query():\n",
        "      response = await query_engine.aquery(\"What’s the main theme?\")\n",
        "      print(response.response)\n",
        "  \n",
        "  asyncio.run(async_query())\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Retrieval Methods**\n",
        "Fetch data from your index without generating a response.\n",
        "\n",
        "### **(a) `retrieve()`**\n",
        "- **Purpose**: Fetch relevant data chunks for a query.\n",
        "- **Analogy**: Using a search engine to get links (not summaries).\n",
        "- **Example**:\n",
        "  ```python\n",
        "  retriever = index.as_retriever(similarity_top_k=3)\n",
        "  nodes = retriever.retrieve(\"What is AI?\")\n",
        "  for node in nodes:\n",
        "      print(node.text)  # Raw text chunks from your data\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **(b) `aretrieve()`**\n",
        "- **Purpose**: Async version of `retrieve()`.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  async def async_retrieve():\n",
        "      nodes = await retriever.aretrieve(\"What is ML?\")\n",
        "      print(nodes)\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Index Construction Methods**\n",
        "Build and manage indexes (structured representations of your data).\n",
        "\n",
        "### **(a) `from_documents()`**\n",
        "- **Purpose**: Create an index from documents (PDFs, text files).\n",
        "- **Example**:\n",
        "  ```python\n",
        "  index = VectorStoreIndex.from_documents(documents)\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **(b) `load_index_from_storage()`**\n",
        "- **Purpose**: Load a pre-saved index (no need to re-index data).\n",
        "- **Example**:\n",
        "  ```python\n",
        "  from llama_index import StorageContext\n",
        "  \n",
        "  storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
        "  index = load_index_from_storage(storage_context)\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Response Synthesis Methods**\n",
        "Combine retrieved data with LLM to generate answers.\n",
        "\n",
        "### **(a) `synthesize()`**\n",
        "- **Purpose**: Generate a response from retrieved nodes.\n",
        "- **Analogy**: Writing an essay using highlighted book passages.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  response = query_engine.synthesize(\n",
        "      query=\"What is Python?\",\n",
        "      nodes=nodes  # Retrieved data\n",
        "  )\n",
        "  print(response.response)\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Advanced Methods**\n",
        "### **(a) Multi-Modal Methods**\n",
        "- **Purpose**: Combine text and images.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  from llama_index.multi_modal_llms import OpenAIMultiModal\n",
        "  \n",
        "  mm_llm = OpenAIMultiModal(model=\"gpt-4-vision-preview\")\n",
        "  response = mm_llm.complete(\"Describe this image:\", image=\"image.jpg\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **(b) Agents**\n",
        "- **Purpose**: LLMs that use tools (calculators, APIs).\n",
        "- **Example**:\n",
        "  ```python\n",
        "  from llama_index.agent import OpenAIAgent\n",
        "  \n",
        "  agent = OpenAIAgent.from_tools(tools=[calculator_tool])\n",
        "  response = agent.chat(\"Calculate 2+2.\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Summary Table: When to Use Which Method**\n",
        "\n",
        "| **Method**       | **Use Case**                                                                 |\n",
        "|------------------|-----------------------------------------------------------------------------|\n",
        "| `complete()`     | Single-turn Q&A, code generation.                                           |\n",
        "| `chat()`         | Multi-turn conversations (e.g., chatbots).                                  |\n",
        "| `stream()`       | Real-time streaming (e.g., typing animations).                              |\n",
        "| `query()`        | Querying indexed data (e.g., document QA).                                  |\n",
        "| `retrieve()`     | Fetching raw data chunks (e.g., search without summaries).                  |\n",
        "| `synthesize()`   | Generating answers from retrieved data (e.g., RAG).                         |\n",
        "| Async Methods    | Building web apps/APIs.                                                     |\n",
        "| Agents           | Complex tasks requiring tools (e.g., math, web searches).                   |\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Parameters Deep Dive**\n",
        "### Common Parameters Across Methods:\n",
        "1. **`temperature`**:\n",
        "   - `0.0`: Factual responses (e.g., Q&A).\n",
        "   - `0.7`: Balanced creativity (e.g., stories).\n",
        "   - `1.0`: Maximum randomness.\n",
        "\n",
        "2. **`max_tokens`**:\n",
        "   - Limits response length (e.g., `max_tokens=100` for short answers).\n",
        "\n",
        "3. **`top_p`**:\n",
        "   - Controls diversity (e.g., `top_p=0.9` for focused responses).\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Full Workflow Example**\n",
        "Let’s build a **RAG pipeline** using all methods:\n",
        "```python\n",
        "# Step 1: Load data\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# Step 2: Build index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Step 3: Retrieve nodes\n",
        "retriever = index.as_retriever()\n",
        "nodes = retriever.retrieve(\"What is climate change?\")\n",
        "\n",
        "# Step 4: Synthesize a response\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.synthesize(query=\"What is climate change?\", nodes=nodes)\n",
        "print(response.response)\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UUbYTZYmqRjv"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llamaindex-course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}